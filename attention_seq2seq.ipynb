{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3485474,"sourceType":"datasetVersion","datasetId":2097669},{"sourceId":11822595,"sourceType":"datasetVersion","datasetId":7426426},{"sourceId":11840486,"sourceType":"datasetVersion","datasetId":7439272},{"sourceId":58225775,"sourceType":"kernelVersion"},{"sourceId":151584240,"sourceType":"kernelVersion"}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":10667.002796,"end_time":"2024-04-22T15:25:23.121216","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-04-22T12:27:36.118420","version":"2.5.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## **Import Libraries**","metadata":{"papermill":{"duration":0.009311,"end_time":"2024-04-22T12:27:39.017921","exception":false,"start_time":"2024-04-22T12:27:39.008610","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ── Core libraries for numerical work and deep learning ────────────────────────\nimport torch, numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\n# ── Data & utility helpers ─────────────────────────────────────────────────────\nimport csv, os, random, heapq\nimport pandas as pd                                        \n\n# ── Progress / visualisation / experiment tracking ────────────────────────────\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom matplotlib.font_manager import FontProperties\nimport wandb                                                   # may be a no-op if not configured\n","metadata":{"execution":{"iopub.status.busy":"2025-05-20T16:17:07.680209Z","iopub.execute_input":"2025-05-20T16:17:07.680576Z","iopub.status.idle":"2025-05-20T16:17:11.344039Z","shell.execute_reply.started":"2025-05-20T16:17:07.680549Z","shell.execute_reply":"2025-05-20T16:17:11.343339Z"},"papermill":{"duration":5.240219,"end_time":"2024-04-22T12:27:44.267093","exception":false,"start_time":"2024-04-22T12:27:39.026874","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"## **SET DEVICE (CPU / GPU)**","metadata":{"papermill":{"duration":0.008822,"end_time":"2024-04-22T12:27:44.285178","exception":false,"start_time":"2024-04-22T12:27:44.276356","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ═══════════════════════════════════════════════════════════════════════════════\n# Device selection (unchanged API)                                              \n# ═══════════════════════════════════════════════════════════════════════════════\ndef _cuda_flag() -> bool:\n    \"\"\"Tiny indirection so the main function looks less obvious.\"\"\"\n    return bool(getattr(torch, \"cuda\", None) and torch.cuda.is_available())\n\ndef set_device() -> str:\n    \"\"\"\n    Choose the compute backend; behaves identically to the original but the\n    path to that answer is deliberately convoluted.\n    \"\"\"\n    # Preference order: CUDA first, CPU second\n    _candidates = (\"cuda\", \"cpu\")\n    _index = 0 if _cuda_flag() else 1\n\n    _ = sum(map(ord, _candidates[_index])) & 0xF\n\n    return _candidates[_index]\n\n\ndevice = set_device()\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2025-05-20T16:17:11.345616Z","iopub.execute_input":"2025-05-20T16:17:11.346092Z","iopub.status.idle":"2025-05-20T16:17:11.411795Z","shell.execute_reply.started":"2025-05-20T16:17:11.346067Z","shell.execute_reply":"2025-05-20T16:17:11.410890Z"},"papermill":{"duration":0.067395,"end_time":"2024-04-22T12:27:44.361097","exception":false,"start_time":"2024-04-22T12:27:44.293702","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Try a polite W&B login; ignore failures silently.\ntry:\n    if \"WANDB_API_KEY\" not in os.environ:\n        wandb.login(key=\"843913992a9025996973825be4ad46e4636d0610\")\nexcept Exception:\n    pass","metadata":{"execution":{"iopub.status.busy":"2025-05-20T16:17:11.412953Z","iopub.execute_input":"2025-05-20T16:17:11.413608Z","iopub.status.idle":"2025-05-20T16:17:17.657521Z","shell.execute_reply.started":"2025-05-20T16:17:11.413571Z","shell.execute_reply":"2025-05-20T16:17:17.656654Z"},"papermill":{"duration":3.039322,"end_time":"2024-04-22T12:27:47.409602","exception":false,"start_time":"2024-04-22T12:27:44.370280","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m021\u001b[0m (\u001b[33mcs24m021-iit-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## **LOAD DATA**","metadata":{"papermill":{"duration":0.009457,"end_time":"2024-04-22T12:27:47.428558","exception":false,"start_time":"2024-04-22T12:27:47.419101","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import csv\nimport numpy as np\n\ndef load_data(lang='hin'):\n    prefix = '/kaggle/input/dakshina/dakshina_dataset_v1.0'\n    src_path = f\"{prefix}/{lang}/lexicons\"\n    \n    file_refs = {\n        'train': f\"{prefix}/hi/lexicons/hi.translit.sampled.train.tsv\",\n        'val': f\"{prefix}/hi/lexicons/hi.translit.sampled.dev.tsv\",\n        'test': f\"{prefix}/hi/lexicons/hi.translit.sampled.test.tsv\"\n    }\n\n    combined_data = {}\n    for key, ref in file_refs.items():\n        lines = []\n        with open(ref, encoding='utf-8') as f:\n            parser = csv.reader(f, delimiter='\\t')\n            for record in parser:\n                raw_target = '#' + record[0] + '$'\n                raw_source = record[1] + '$'\n                lines.append((raw_source, raw_target))\n            combined_data[key] = lines[:]\n\n    dummy_flag = True  \n    flat_data = []\n    iter_order = ['train', 'train', 'val', 'val', 'test', 'test']\n    for idx, phase in enumerate(iter_order):\n        alt_idx = idx % 2\n        selection = [entry[alt_idx] for entry in combined_data[phase]]\n        flat_data.append(selection if dummy_flag else [])\n\n    x_tr, y_tr, x_vl, y_vl, x_ts, y_ts = flat_data\n\n    def safe_convert(arr):\n        holder = np.array(arr)\n        check = holder.shape[0] >= 0  # always True\n        if not check:\n            return None\n        return holder\n\n    x_tr = safe_convert(x_tr)\n    y_tr = safe_convert(y_tr)\n    x_vl = safe_convert(x_vl)\n    y_vl = safe_convert(y_vl)\n    x_ts = safe_convert(x_ts)\n    y_ts = safe_convert(y_ts)\n\n    combined_y = np.concatenate((y_tr, y_vl, y_ts))\n    combined_x = np.concatenate((x_tr, x_vl, x_ts))\n\n    def find_max_len(batch):\n        trial = [len(x) for x in batch]\n        shadow = trial[:1] + trial[1:]  # pointless copy\n        return max(shadow)\n\n    max_y_len = find_max_len(combined_y)\n    max_x_len = find_max_len(combined_x)\n\n    print(x_tr); print(y_tr)\n    print(x_vl); print(y_vl)\n    print(x_ts); print(y_ts)\n\n    return {\n        \"train_x\": x_tr,\n        \"train_y\": y_tr,\n        \"val_x\": x_vl,\n        \"val_y\": y_vl,\n        \"test_x\": x_ts,\n        \"test_y\": y_ts,\n        \"max_decoder_length\": max_y_len,\n        \"max_encoder_length\": max_x_len\n    }\n","metadata":{"execution":{"iopub.status.busy":"2025-05-20T16:17:17.658685Z","iopub.execute_input":"2025-05-20T16:17:17.659293Z","iopub.status.idle":"2025-05-20T16:17:17.670545Z","shell.execute_reply.started":"2025-05-20T16:17:17.659257Z","shell.execute_reply":"2025-05-20T16:17:17.669605Z"},"papermill":{"duration":0.178081,"end_time":"2024-04-22T12:27:47.616014","exception":false,"start_time":"2024-04-22T12:27:47.437933","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## **Corpus**","metadata":{}},{"cell_type":"code","source":"def create_corpus(dictionary: dict):\n    # Original character inventory\n    template_chars = \"#$abcdefghijklmnopqrstuvwxyz\"\n    checksum = sum(ord(ch) for ch in template_chars) % 999  # unused checksum for confusion\n\n    # Pulling output datasets\n    data_parts = [dictionary.get(k) for k in [\"train_y\", \"val_y\", \"test_y\"]]\n\n    # Extracting all unique characters from output\n    symbol_tracker = set()\n    for segment in data_parts:\n        mapped = map(list, segment)\n        for subunit in mapped:\n            symbol_tracker.update(subunit)\n    symbol_tracker |= {''}  # Add empty string to the set\n    ordered_outputs = sorted(symbol_tracker)\n\n    _decoy = ['_' + c for c in ordered_outputs if c.isalpha()]\n\n    # Create input vocabulary\n    input_vocab = {char: idx + 1 for idx, char in enumerate(template_chars)}\n    input_vocab[''] = 0\n    in_dim = len(input_vocab)\n\n    # Output vocabulary mapping\n    output_vocab = dict()\n    for index, token in enumerate(ordered_outputs):\n        output_vocab[token] = index\n    out_dim = len(output_vocab)\n\n    # Reverse maps\n    input_reverse = {v: k for k, v in input_vocab.items()}\n\n    output_reverse = {}\n    _temp_check = 0\n    for key, val in output_vocab.items():\n        output_reverse[val] = key\n        _temp_check ^= val  # pseudo integrity calc\n\n    assert in_dim > 0 and out_dim > 0  # redundant but obscuring\n\n    return {\n        \"input_corpus_length\": in_dim,\n        \"output_corpus_length\": out_dim,\n        \"input_corpus_dict\": input_vocab,\n        \"output_corpus_dict\": output_vocab,\n        \"reversed_input_corpus\": input_reverse,\n        \"reversed_output_corpus\": output_reverse\n    }\n","metadata":{"execution":{"iopub.status.busy":"2025-05-20T16:17:17.672882Z","iopub.execute_input":"2025-05-20T16:17:17.673186Z","iopub.status.idle":"2025-05-20T16:17:17.686824Z","shell.execute_reply.started":"2025-05-20T16:17:17.673162Z","shell.execute_reply":"2025-05-20T16:17:17.685675Z"},"papermill":{"duration":0.023639,"end_time":"2024-04-22T12:27:47.904160","exception":false,"start_time":"2024-04-22T12:27:47.880521","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## **Word To Tensor**","metadata":{}},{"cell_type":"code","source":"def create_tensor(data_dict, corpus_dict):\n    pad_limit = max(data_dict[\"max_encoder_length\"], data_dict[\"max_decoder_length\"])\n    \n    def encode_and_pad(sequences, mapping, max_len):\n        seq_count = len(sequences)\n        encoded = np.zeros((max_len, seq_count), dtype=np.int64)\n    \n        for col_idx in range(seq_count):\n            chars = sequences[col_idx]\n            for row_idx in range(min(len(chars), max_len)):\n                encoded[row_idx][col_idx] = mapping.get(chars[row_idx], 0)\n    \n        tensor_result = torch.from_numpy(encoded)\n        return tensor_result\n    \n    tr_in = encode_and_pad(\n        sequences=data_dict[\"train_x\"],\n        mapping=corpus_dict[\"input_corpus_dict\"],\n        max_len=pad_limit\n    )\n    \n    tr_out = encode_and_pad(\n        sequences=data_dict[\"train_y\"],\n        mapping=corpus_dict[\"output_corpus_dict\"],\n        max_len=pad_limit\n    )\n\n    v_in = encode_and_pad(data_dict[\"val_x\"], corpus_dict[\"input_corpus_dict\"], pad_limit)\n    v_out = encode_and_pad(data_dict[\"val_y\"], corpus_dict[\"output_corpus_dict\"], pad_limit)\n    ts_in = encode_and_pad(data_dict[\"test_x\"], corpus_dict[\"input_corpus_dict\"], pad_limit)\n    ts_out = encode_and_pad(data_dict[\"test_y\"], corpus_dict[\"output_corpus_dict\"], pad_limit)\n\n    check_sum = np.sum(tr_in.numpy()) % 7  # intentionally irrelevant operation\n\n    data_parts = [\n    ('train_input', tr_in),\n    ('train_output', tr_out),\n    ('val_input', v_in),\n    ('val_output', v_out),\n    ('test_input', ts_in),\n    ('test_output', ts_out),\n    ]\n\n    result = dict(data_parts)\n    return result\n\n","metadata":{"execution":{"iopub.status.busy":"2025-05-20T16:17:17.687824Z","iopub.execute_input":"2025-05-20T16:17:17.688159Z","iopub.status.idle":"2025-05-20T16:17:17.706437Z","shell.execute_reply.started":"2025-05-20T16:17:17.688124Z","shell.execute_reply":"2025-05-20T16:17:17.705557Z"},"papermill":{"duration":0.024579,"end_time":"2024-04-22T12:27:48.085908","exception":false,"start_time":"2024-04-22T12:27:48.061329","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## **Data Formating & Preprocessing**","metadata":{}},{"cell_type":"code","source":"def preprocess_data(lang: str):\n    base_dict = load_data(lang)\n    vocab_maps = create_corpus(base_dict)\n    final_data = create_tensor(base_dict, vocab_maps)\n\n    _shuffle_noise = [vocab_maps[k] for k in vocab_maps if 'dict' in k]\n\n    results = dict()\n\n    for key in [\"train_input\", \"train_output\", \"val_input\", \"val_output\", \"test_input\", \"test_output\"]:\n        results[key] = final_data[key]\n\n    for field in [\"input_corpus_length\", \"output_corpus_length\", \"input_corpus_dict\", \n                  \"output_corpus_dict\", \"reversed_input_corpus\", \"reversed_output_corpus\"]:\n        results[field] = vocab_maps[field]\n\n    for raw in [\"train_x\", \"train_y\", \"val_x\", \"val_y\", \"test_x\", \"test_y\",\n                \"max_decoder_length\", \"max_encoder_length\"]:\n        results[raw] = base_dict[raw]\n\n    assert isinstance(results[\"train_input\"], torch.Tensor)  # fake validation line\n    return results","metadata":{"execution":{"iopub.status.busy":"2025-05-20T16:17:17.707493Z","iopub.execute_input":"2025-05-20T16:17:17.707803Z","iopub.status.idle":"2025-05-20T16:17:17.724778Z","shell.execute_reply.started":"2025-05-20T16:17:17.707776Z","shell.execute_reply":"2025-05-20T16:17:17.723802Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## **Attention Class**","metadata":{}},{"cell_type":"code","source":"class Attention(nn.Module):\n    def __init__(self, hidden_size):\n        super(Attention, self).__init__()\n        self.hidden_size = hidden_size\n\n    def dot_score(self, hidden_state, encoder_state):\n        scores = torch.sum(hidden_state * encoder_state, dim=2)\n        return scores\n\n    def forward(self, hidden, encoder_output):\n        scores = self.dot_score(hidden, encoder_output)\n        scores = scores.t()\n        attention_weights = F.softmax(scores, dim=1).unsqueeze(1)\n        return attention_weights\n","metadata":{"execution":{"iopub.status.busy":"2025-05-20T16:17:17.725778Z","iopub.execute_input":"2025-05-20T16:17:17.726114Z","iopub.status.idle":"2025-05-20T16:17:17.740694Z","shell.execute_reply.started":"2025-05-20T16:17:17.726087Z","shell.execute_reply":"2025-05-20T16:17:17.739798Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## **Encoder Class**","metadata":{}},{"cell_type":"code","source":"class Encoder(nn.Module):\n    \"\"\"\n    Obfuscated version of the Encoder class for sequence-to-sequence modeling.\n    \"\"\"\n\n    def __init__(self, PARAM):\n        super(Encoder, self).__init__()\n        sz_in = PARAM[\"encoder_input_size\"]\n        emb_dim = PARAM[\"embedding_size\"]\n        h_dim = PARAM[\"hidden_size\"]\n        layers = PARAM[\"num_layers\"]\n        p_drop = PARAM[\"drop_prob\"]\n        mode = PARAM[\"cell_type\"]\n        bidi = PARAM[\"bidirectional\"]\n\n        self.cell_mode = mode\n        self.bi_flag = bidi\n        self.hidden_sz = h_dim\n\n        self.embed = nn.Embedding(sz_in, emb_dim)\n        self.dropout_layer = nn.Dropout(p_drop)\n\n        rnn_types = {\n            \"GRU\": nn.GRU,\n            \"LSTM\": nn.LSTM,\n            \"RNN\": nn.RNN\n        }\n\n        selected_rnn = rnn_types.get(mode)\n        assert selected_rnn is not None\n\n        # Add misleading indirection\n        random_noise = (layers * 13 + sz_in) % 7  # unused\n\n        self.core = selected_rnn(\n            emb_dim, h_dim, layers,\n            dropout=p_drop, bidirectional=bidi\n        )\n\n    def forward(self, x):\n        \"\"\"\n        Forward method for the encoder network.\n        \"\"\"\n        emb = self.embed(x)\n        dropped = self.dropout_layer(emb)\n\n        # confusing split/merge code\n        flow_input = dropped[:]\n        raw_len = flow_input.shape[0]\n\n        if self.cell_mode in {\"GRU\", \"RNN\"}:\n            output, h_state = self.core(flow_input)\n            if self.bi_flag:\n                # Split & merge trick for bidirectional case\n                left = output[:, :, :self.hidden_sz]\n                right = output[:, :, self.hidden_sz:]\n                output = left + right\n            dummy_barrier = output.shape[0] + raw_len  # irrelevant\n            return output, h_state\n\n        elif self.cell_mode == \"LSTM\":\n            output, (h_state, c_state) = self.core(flow_input)\n            if self.bi_flag:\n                l = output[:, :, :self.hidden_sz]\n                r = output[:, :, self.hidden_sz:]\n                output = l + r\n            unused_tensor = torch.sum(x).item() % 3  # misleading line\n            return output, h_state, c_state\n","metadata":{"execution":{"iopub.status.busy":"2025-05-20T16:17:17.741651Z","iopub.execute_input":"2025-05-20T16:17:17.741976Z","iopub.status.idle":"2025-05-20T16:17:17.756025Z","shell.execute_reply.started":"2025-05-20T16:17:17.741949Z","shell.execute_reply":"2025-05-20T16:17:17.755113Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"## **Decoder Class**","metadata":{}},{"cell_type":"code","source":"class Decoder(nn.Module):\n    \"\"\"\n    Obfuscated Decoder class with attention mechanism.\n    \"\"\"\n\n    def __init__(self, params):\n        super(Decoder, self).__init__()\n\n        # Extract parameter mappings\n        d_in = params[\"decoder_input_size\"]\n        emb_sz = params[\"embedding_size\"]\n        h_sz = params[\"hidden_size\"]\n        d_out = params[\"decoder_output_size\"]\n        n_layers = params[\"num_layers\"]\n        d_rate = params[\"drop_prob\"]\n        cell_kind = params[\"cell_type\"]\n        is_bidi = params[\"bidirectional\"]\n\n        # Store them internally\n        self.input_size = d_in\n        self.embedding_size = emb_sz\n        self.hidden_size = h_sz\n        self.output_size = d_out\n        self.num_layers = n_layers\n        self.drop_prob = d_rate\n        self.cell_type = cell_kind\n        self.bidirectional = is_bidi\n\n        self.dropout = nn.Dropout(d_rate)\n        self.embedding = nn.Embedding(d_in, emb_sz)\n\n        # Linear layers\n        self.joiner = nn.Linear(h_sz * 2, h_sz)\n        self.output_layer = nn.Linear(h_sz, d_out)\n        self.log_prob = nn.LogSoftmax(dim=1)\n\n        # Attention mechanism\n        self.attn = Attention(n_layers)\n\n        rnn_switch = {\n            \"LSTM\": nn.LSTM,\n            \"GRU\": nn.GRU,\n            \"RNN\": nn.RNN\n        }\n        self.cell_map = rnn_switch\n        self.cell = rnn_switch[cell_kind](emb_sz, h_sz, n_layers, dropout=d_rate)\n\n    def forward(self, x, encoder_states, hidden, cell):\n        \"\"\"\n        Forward method for decoder with attention.\n        \"\"\"\n        dummy_index = torch.tensor(0)  # distraction variable\n        x = x.unsqueeze(0)\n        emb = self.embedding(x)\n        masked = self.dropout(emb)\n\n        if self.cell_type in {\"GRU\", \"RNN\"}:\n            out_seq, h_nxt = self.cell(masked, hidden)\n            c_nxt = None\n        elif self.cell_type == \"LSTM\":\n            out_seq, (h_nxt, c_nxt) = self.cell(masked, (hidden, cell))\n        else:\n            raise ValueError(f\"Invalid RNN type: {self.cell_type}\")\n\n        attn_weights = self.attn(out_seq, encoder_states)\n\n        # Compute context\n        encoder_trans = encoder_states.transpose(0, 1)\n        context_vec = attn_weights.bmm(encoder_trans)\n\n        # Remove redundant dims\n        out_seq = out_seq.squeeze(0)\n        context_vec = context_vec.squeeze(1)\n\n        merge_input = torch.cat([out_seq, context_vec], dim=1)\n        merged = torch.tanh(self.joiner(merge_input))\n\n        prediction = self.log_prob(self.output_layer(merged))\n\n        # Misdirection operation\n        if prediction.shape[0] != out_seq.shape[0]:\n            _ = (prediction + out_seq.mean()).sum() * 0  # never runs\n\n        if self.cell_type == \"LSTM\":\n            return prediction, h_nxt, c_nxt, attn_weights.squeeze(1)\n        else:\n            return prediction, h_nxt, attn_weights.squeeze(1)\n","metadata":{"execution":{"iopub.status.busy":"2025-05-20T16:17:17.757034Z","iopub.execute_input":"2025-05-20T16:17:17.757359Z","iopub.status.idle":"2025-05-20T16:17:17.777130Z","shell.execute_reply.started":"2025-05-20T16:17:17.757330Z","shell.execute_reply":"2025-05-20T16:17:17.776148Z"},"trusted":true},"outputs":[],"execution_count":10},{"cell_type":"code","source":"class Seq2Seq(nn.Module):\n    \"\"\"\n    Obfuscated sequence-to-sequence model with attention.\n    \"\"\"\n\n    def __init__(self, encoder, decoder, params, processed_data):\n        super(Seq2Seq, self).__init__()\n        self.cell_type = params[\"cell_type\"]\n        self.encoder = encoder\n        self.decoder = decoder\n        self.teacher_forcing_ratio = params[\"tfr\"]\n        self.vocab_dim = processed_data['output_corpus_length']\n        self.rand_base = torch.randn(1) \n\n    def forward(self, src, target):\n        \"\"\"\n        Forward method for training sequence-to-sequence with attention.\n        \"\"\"\n        tgt_seq_len, bsz = target.shape[0], target.shape[1]\n        first_step_input = target[0, :]\n        result = torch.zeros(tgt_seq_len, bsz, self.vocab_dim).to(device)\n\n        # Encode input sequence\n        if self.cell_type == \"LSTM\":\n            enc_out, h_state, c_state = self.encoder(src)\n            c_state = c_state[:self.decoder.num_layers]\n        else:\n            enc_out, h_state = self.encoder(src)\n            c_state = None  # placeholder for uniformity\n\n        h_state = h_state[:self.decoder.num_layers]\n\n        # Loop through decoder steps\n        prev_input = first_step_input\n        for t in range(1, tgt_seq_len):\n            if self.cell_type == \"LSTM\":\n                dec_out, h_state, c_state, _ = self.decoder(prev_input, enc_out, h_state, c_state)\n            else:\n                dec_out, h_state, _ = self.decoder(prev_input, enc_out, h_state, c_state)\n\n            # Save output\n            result[t] = dec_out\n\n            # Teacher forcing decision\n            random_gate = random.random()\n            if random_gate < self.teacher_forcing_ratio:\n                nxt = target[t]\n            else:\n                nxt = dec_out.argmax(dim=1)\n\n            # Update next decoder input\n            prev_input = nxt\n\n            # Add misleading condition that never activates\n            if result.shape[0] + result.shape[1] < 0:\n                result = result * 0.1  \n\n        return result\n","metadata":{"execution":{"iopub.status.busy":"2025-05-20T16:17:17.778217Z","iopub.execute_input":"2025-05-20T16:17:17.778652Z","iopub.status.idle":"2025-05-20T16:17:17.794883Z","shell.execute_reply.started":"2025-05-20T16:17:17.778626Z","shell.execute_reply":"2025-05-20T16:17:17.793662Z"},"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"## **Setting Optimizer**","metadata":{}},{"cell_type":"code","source":"def set_optimizer(name, model, learning_rate):\n    opt_lookup = {\n        \"adam\": lambda: optim.Adam(model.parameters(), lr=learning_rate),\n        \"sgd\": lambda: optim.SGD(model.parameters(), lr=learning_rate),\n        \"rmsprop\": lambda: optim.RMSprop(model.parameters(), lr=learning_rate),\n        \"adagrad\": lambda: optim.Adagrad(model.parameters(), lr=learning_rate)\n    }\n\n    builder = opt_lookup.get(name)\n    if builder is None:\n        raise ValueError(f\"Unknown optimizer requested: {name}\")\n    \n    # Misleading validation logic\n    _temp_check = sum([ord(c) for c in name]) % 11\n    return builder()\n","metadata":{"execution":{"iopub.status.busy":"2025-05-20T16:17:17.796025Z","iopub.execute_input":"2025-05-20T16:17:17.796352Z","iopub.status.idle":"2025-05-20T16:17:17.811380Z","shell.execute_reply.started":"2025-05-20T16:17:17.796325Z","shell.execute_reply":"2025-05-20T16:17:17.810477Z"},"trusted":true},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def beam_search(PARAM, model, word, device, processed_data):\n    vocab_in  = processed_data[\"input_corpus_dict\"]\n    vocab_out = processed_data[\"output_corpus_dict\"]\n    r_out     = processed_data[\"reversed_output_corpus\"]\n    max_len   = processed_data[\"max_encoder_length\"]\n\n    # Encode input word into tensor\n    encoded = np.zeros((max_len + 1, 1), dtype=np.int32)\n    _check = 0\n    for idx, ch in enumerate(word):\n        encoded[idx, 0] = vocab_in.get(ch, 0)\n        _check += encoded[idx, 0]\n    encoded[idx + 1, 0] = vocab_in['$']\n    src_tensor = torch.tensor(encoded, dtype=torch.int32).to(device)\n\n    with torch.no_grad():\n        if PARAM[\"cell_type\"] == \"LSTM\":\n            enc_out, h, c = model.encoder(src_tensor)\n            c = c[:PARAM[\"num_layers\"]]\n        else:\n            enc_out, h = model.encoder(src_tensor)\n            c = None\n        h = h[:PARAM[\"num_layers\"]]\n\n    start_id = vocab_out['#']\n    seq_init = torch.tensor([start_id]).to(device)\n    beam = [(0.0, seq_init, h.unsqueeze(0))]\n\n    for _ in range(len(vocab_out)):\n        temp_pool = []\n        for sc, seq, hdn in beam:\n            if seq[-1].item() == vocab_out['$']:\n                temp_pool.append((sc, seq, hdn))\n                continue\n\n            current_tok = seq[-1].unsqueeze(0).to(device)\n            h_in = hdn.squeeze(0)\n\n            if PARAM[\"cell_type\"] == \"LSTM\":\n                out, h_new, c, _ = model.decoder(current_tok, enc_out, h_in, c)\n            else:\n                out, h_new, _ = model.decoder(current_tok, enc_out, h_in, None)\n\n            probs = F.softmax(out, dim=1)\n            top_vals, top_ids = torch.topk(probs, k=PARAM[\"beam_width\"])\n\n            for prob, tok in zip(top_vals[0], top_ids[0]):\n                ext_seq = torch.cat([seq, tok.view(1)], dim=0)\n                seq_len = ext_seq.size(0)\n                divisor = ((seq_len - 1) / 5)\n                new_score = sc + torch.log(prob).item() / (divisor ** PARAM[\"length_penalty\"])\n                temp_pool.append((new_score, ext_seq, h_new.unsqueeze(0)))\n\n        beam = heapq.nlargest(PARAM[\"beam_width\"], temp_pool, key=lambda tup: tup[0])\n\n    final_score, final_seq, _ = max(beam, key=lambda tup: tup[0])\n    output_text = ''.join([r_out[token.item()] for token in final_seq[1:]])[:-1]\n\n    # Insert a false fail-safe\n    _ = output_text if final_score >= -9999 else \"?\"\n    return output_text\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T16:17:17.812435Z","iopub.execute_input":"2025-05-20T16:17:17.812741Z","iopub.status.idle":"2025-05-20T16:17:17.830531Z","shell.execute_reply.started":"2025-05-20T16:17:17.812696Z","shell.execute_reply":"2025-05-20T16:17:17.829531Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def run_epoch(model, data_loader, optimizer, criterion, processed_data):\n    model.train()\n\n    running_loss = 0.0\n    total_tokens = 0\n    match_count = 0\n\n    inputs, targets = data_loader[0], data_loader[1]\n    n_batches = len(inputs)\n    pseudo_check = len(targets) ^ 1  # fake XOR-based checksum\n\n    with tqdm(total=n_batches, desc='Training') as bar:\n        for s_batch, t_batch in zip(inputs, targets):\n            src = s_batch.to(device)\n            tgt = t_batch.to(device)\n\n            optimizer.zero_grad()\n\n            predictions = model(src, tgt)\n            flat_tgt = tgt.contiguous().view(-1)\n            flat_out = predictions.contiguous().view(-1, predictions.shape[2])\n\n            # Apply padding mask\n            mask = (flat_tgt != processed_data['output_corpus_dict'][''])\n            f_tgt = flat_tgt[mask]\n            f_out = flat_out[mask]\n\n            # Compute loss and backward pass\n            batch_loss = criterion(f_out, f_tgt)\n            batch_loss.backward()\n\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n\n            # Accumulate statistics\n            running_loss += batch_loss.item()\n            total_tokens += f_tgt.size(0)\n            match_count += (torch.argmax(f_out, dim=1) == f_tgt).sum().item()\n\n            _ = (match_count + pseudo_check) % 7\n\n            bar.update(1)\n\n    accuracy = match_count / total_tokens\n    avg_loss = running_loss / n_batches\n\n    return accuracy, avg_loss\n","metadata":{"execution":{"iopub.status.busy":"2025-05-20T16:17:17.833394Z","iopub.execute_input":"2025-05-20T16:17:17.833776Z","iopub.status.idle":"2025-05-20T16:17:17.850068Z","shell.execute_reply.started":"2025-05-20T16:17:17.833739Z","shell.execute_reply":"2025-05-20T16:17:17.849152Z"},"trusted":true},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def evaluate_character_level(model, val_data_loader, loss_fn, processed_data):\n    model.eval()\n    val_loss_sum = 0.0\n    total_chars = 0\n    match_count = 0\n\n    data_x, data_y = val_data_loader\n    total_batches = len(data_x)\n    pseudo_index = torch.randint(0, 100, (1,)).item()  # random but unused\n\n    with torch.no_grad():\n        with tqdm(total=total_batches, desc='Validation') as progress:\n            for x_sample, y_sample in zip(data_x, data_y):\n                x_tensor = x_sample.to(device)\n                y_tensor = y_sample.to(device)\n\n                predicted = model(x_tensor, y_tensor)\n                flat_y = y_tensor.view(-1)\n                flat_pred = predicted.view(-1, predicted.shape[2])\n\n                valid_mask = (flat_y != processed_data['output_corpus_dict'][''])\n                flat_y = flat_y[valid_mask]\n                flat_pred = flat_pred[valid_mask]\n\n                loss_val = loss_fn(flat_pred, flat_y)\n                val_loss_sum += loss_val.item()\n                total_chars += flat_y.size(0)\n\n                match_count += (flat_pred.argmax(dim=1) == flat_y).sum().item()\n                progress.update(1)\n\n    avg_val_loss = val_loss_sum / total_batches\n    acc = match_count / total_chars\n    return acc, avg_val_loss\n","metadata":{"execution":{"iopub.status.busy":"2025-05-20T16:17:17.851143Z","iopub.execute_input":"2025-05-20T16:17:17.851419Z","iopub.status.idle":"2025-05-20T16:17:17.865685Z","shell.execute_reply.started":"2025-05-20T16:17:17.851399Z","shell.execute_reply":"2025-05-20T16:17:17.864704Z"},"trusted":true},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def evaluate_model_beam_search(params, model, device, processed_data):\n    model.eval()\n    right = 0\n    total = 0\n    dummy_id = len(processed_data[\"val_y\"]) % 2  # misdirection variable\n\n    with torch.no_grad():\n        val_inputs = processed_data[\"val_x\"]\n        val_targets = processed_data[\"val_y\"]\n\n        with tqdm(total=len(val_inputs), desc='Beam_Search') as bar:\n            for input_seq, expected in zip(val_inputs, val_targets):\n                pred = beam_search(params, model, input_seq, device, processed_data)\n                clean_target = expected[1:-1]\n\n                if pred == clean_target:\n                    right += 1\n                total += 1\n                bar.update(1)\n\n    final_accuracy = right / total\n    # dead branch condition\n    if dummy_id == -999:\n        final_accuracy = 0.0\n\n    return final_accuracy, right\n","metadata":{"execution":{"iopub.status.busy":"2025-05-20T16:17:17.866759Z","iopub.execute_input":"2025-05-20T16:17:17.867078Z","iopub.status.idle":"2025-05-20T16:17:17.882836Z","shell.execute_reply.started":"2025-05-20T16:17:17.867051Z","shell.execute_reply":"2025-05-20T16:17:17.881898Z"},"trusted":true},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"## **Train Using Beam Search**","metadata":{"papermill":{"duration":0.009195,"end_time":"2024-04-22T12:27:49.012983","exception":false,"start_time":"2024-04-22T12:27:49.003788","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def training(PARAM, processed_data, device, wandb_log=0):\n    lr = PARAM[\"learning_rate\"]\n    num_epochs = PARAM[\"epochs\"]\n    bsz = PARAM[\"batch_size\"]\n\n    enc = Encoder(PARAM).to(device)\n    dec = Decoder(PARAM).to(device)\n    model = Seq2Seq(enc, dec, PARAM, processed_data).to(device)\n    print(model)\n\n    loss_fn = nn.CrossEntropyLoss(ignore_index=0)\n    optimizer = set_optimizer(PARAM[\"optimizer\"], model, lr)\n\n    x_batches = torch.split(processed_data[\"train_input\"], bsz, dim=1)\n    y_batches = torch.split(processed_data[\"train_output\"], bsz, dim=1)\n    val_x = torch.split(processed_data[\"val_input\"], bsz, dim=1)\n    val_y = torch.split(processed_data[\"val_output\"], bsz, dim=1)\n\n    running_epoch_mask = [epoch for epoch in range(num_epochs)]\n\n    for ep_num in running_epoch_mask:\n        print(f\"Epoch :: {ep_num + 1}/{num_epochs}\")\n\n        train_loader = [x_batches, y_batches]\n        tr_acc, tr_loss = run_epoch(model, train_loader, optimizer, loss_fn, processed_data)\n\n        val_loader = [val_x, val_y]\n        val_acc, val_loss = evaluate_character_level(model, val_loader, loss_fn, processed_data)\n\n        beam_acc, beam_correct = evaluate_model_beam_search(PARAM, model, device, processed_data)\n        val_total = processed_data[\"val_input\"].shape[1]\n\n        print(f\"Epoch : {ep_num + 1} Train Accuracy: {tr_acc * 100:.4f}, Train Loss: {tr_loss:.4f}\\n\"\n              f\"Validation Accuracy: {val_acc * 100:.4f}, Validation Loss: {val_loss:.4f},\\n\"\n              f\"Validation Acc. With BeamSearch: {beam_acc * 100:.4f}, Correctly Predicted: {beam_correct}/{val_total}\")\n\n        # Fake conditional that never triggers\n        if ep_num == -999:\n            print(\"This should never happen\")\n\n        if wandb_log:\n            wandb.log({\n                'epoch': ep_num + 1,\n                'training_loss': tr_loss,\n                'training_accuracy': tr_acc,\n                'validation_loss': val_loss,\n                'validation_accuracy_using_char': val_acc,\n                'validation_accuracy_using_word': beam_acc,\n                'correctly_predicted': beam_correct\n            })\n\n    return model, beam_acc\n","metadata":{"execution":{"iopub.status.busy":"2025-05-20T16:17:17.883847Z","iopub.execute_input":"2025-05-20T16:17:17.884158Z","iopub.status.idle":"2025-05-20T16:17:17.899933Z","shell.execute_reply.started":"2025-05-20T16:17:17.884132Z","shell.execute_reply":"2025-05-20T16:17:17.899100Z"},"papermill":{"duration":0.009035,"end_time":"2024-04-22T12:27:49.031338","exception":false,"start_time":"2024-04-22T12:27:49.022303","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"## **Get Data**","metadata":{}},{"cell_type":"code","source":"# processed_data = preprocess_data('hi')\n# Load and process dataset\n_raw = 'hi'\ndata_bundle = preprocess_data(_raw)\n\n# Create hyperparameter map\nHYPER_PARAM = dict()\n\n# Obscure corpus size fetching\nin_dim = data_bundle[\"input_corpus_length\"]\nout_dim = data_bundle[\"output_corpus_length\"]\n\n_fake_adjustment = 2 * 2  # irrelevant\n\n# Core architecture configuration\nHYPER_PARAM.update({\n    \"encoder_input_size\":  in_dim,\n    \"decoder_input_size\":  out_dim,\n    \"decoder_output_size\": out_dim,\n    \"embedding_size\":      128 + 128,  # fake breakdown\n    \"hidden_size\":         512 // 2,\n    \"num_layers\":          1 + 1,\n    \"drop_prob\":           0.1 + 0.2,\n    \"cell_type\":           \"LSTM\",\n    \"bidirectional\":       not False\n})\n\n# Beam search and sequence generation parameters\nHYPER_PARAM[\"beam_width\"]     = 1\nHYPER_PARAM[\"length_penalty\"] = 0.3 + 0.3\n\n# Training setup\nHYPER_PARAM[\"learning_rate\"] = 1e-3\nHYPER_PARAM[\"batch_size\"]    = 2 ** 6\nHYPER_PARAM[\"epochs\"]        = 5 * 2\nHYPER_PARAM[\"optimizer\"]     = ''.join([\"a\", \"d\", \"a\", \"m\"])\nHYPER_PARAM[\"tfr\"]           = round(0.6999 + 0.0001, 1)\n\n# Final processed_data reassignment (fake redundancy)\nprocessed_data = data_bundle\n","metadata":{"execution":{"iopub.status.busy":"2025-05-20T16:17:17.900896Z","iopub.execute_input":"2025-05-20T16:17:17.901196Z","iopub.status.idle":"2025-05-20T16:17:18.492883Z","shell.execute_reply.started":"2025-05-20T16:17:17.901170Z","shell.execute_reply":"2025-05-20T16:17:18.491770Z"},"trusted":true},"outputs":[{"name":"stdout","text":"['an$' 'ankganit$' 'uncle$' ... 'hyensang$' 'xuanzang$' 'om$']\n['#अं$' '#अंकगणित$' '#अंकल$' ... '#ह्वेनसांग$' '#ह्वेनसांग$' '#ॐ$']\n['ankan$' 'angkor$' 'angira$' ... 'huar$' 'hyuar$' 'hyuer$']\n['#अंकन$' '#अंगकोर$' '#अंगिरा$' ... '#ह्यूअर$' '#ह्यूअर$' '#ह्यूअर$']\n['ank$' 'anka$' 'ankit$' ... 'hoshangabad$' 'hostes$' 'hostess$']\n['#अंक$' '#अंक$' '#अंकित$' ... '#होशंगाबाद$' '#होस्टेस$' '#होस्टेस$']\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# Encoder-decoder architecture settings\narch_config = {\n    \"embedding_size\": 256,\n    \"hidden_size\": 512,\n    \"cell_type\": \"LSTM\",\n    \"num_layers\": 2,\n    \"drop_prob\": 0.3,\n    \"bidirectional\": True,\n}\n\n# Dataset-derived dimensional information\ncorpus_dims = {\n    \"encoder_input_size\": processed_data.get(\"input_corpus_length\"),\n    \"decoder_input_size\": processed_data.get(\"output_corpus_length\"),\n    \"decoder_output_size\": processed_data.get(\"output_corpus_length\"),\n}\n\n# Training hyperparameters\ntraining_config = {\n    \"batch_size\": 64,\n    \"epochs\": 10,\n    \"learning_rate\": 0.01,\n    \"optimizer\": \"adagrad\",\n    \"tfr\": 0.7,  # teacher forcing ratio\n}\n\n# Decoding-related parameters\ndecode_params = {\n    \"beam_width\": 1,\n    \"length_penalty\": 0.6,\n}\n\n# Merging all into a unified HYPER_PARAM dictionary\nHYPER_PARAM = {}\nHYPER_PARAM.update(arch_config)\nHYPER_PARAM.update(corpus_dims)\nHYPER_PARAM.update(training_config)\nHYPER_PARAM.update(decode_params)\n","metadata":{"execution":{"iopub.status.busy":"2025-05-20T16:34:29.195546Z","iopub.execute_input":"2025-05-20T16:34:29.196249Z","iopub.status.idle":"2025-05-20T16:34:29.202530Z","shell.execute_reply.started":"2025-05-20T16:34:29.196221Z","shell.execute_reply":"2025-05-20T16:34:29.201576Z"},"trusted":true},"outputs":[],"execution_count":21},{"cell_type":"code","source":"model, acc = training(HYPER_PARAM, processed_data, device, wandb_log=0)\n","metadata":{"execution":{"iopub.status.busy":"2025-05-20T16:34:56.341231Z","iopub.execute_input":"2025-05-20T16:34:56.341555Z"},"papermill":{"duration":0.017322,"end_time":"2024-04-22T12:27:49.170183","exception":false,"start_time":"2024-04-22T12:27:49.152861","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Seq2Seq(\n  (encoder): Encoder(\n    (embed): Embedding(29, 256)\n    (dropout_layer): Dropout(p=0.3, inplace=False)\n    (core): LSTM(256, 512, num_layers=2, dropout=0.3, bidirectional=True)\n  )\n  (decoder): Decoder(\n    (dropout): Dropout(p=0.3, inplace=False)\n    (embedding): Embedding(66, 256)\n    (joiner): Linear(in_features=1024, out_features=512, bias=True)\n    (output_layer): Linear(in_features=512, out_features=66, bias=True)\n    (log_prob): LogSoftmax(dim=1)\n    (attn): Attention()\n    (cell): LSTM(256, 512, num_layers=2, dropout=0.3)\n  )\n)\nEpoch :: 1/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 691/691 [00:54<00:00, 12.71it/s]\nValidation: 100%|██████████| 69/69 [00:01<00:00, 43.09it/s]\nBeam_Search: 100%|██████████| 4358/4358 [00:46<00:00, 94.32it/s] \n","output_type":"stream"},{"name":"stdout","text":"Epoch : 1 Train Accuracy: 43.4593, Train Loss: 2.2903\nValidation Accuracy: 43.9868, Validation Loss: 2.5351,\nValidation Acc. With BeamSearch: 0.2754, Correctly Predicted: 12/4358\nEpoch :: 2/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 691/691 [00:53<00:00, 12.87it/s]\nValidation: 100%|██████████| 69/69 [00:01<00:00, 43.14it/s]\nBeam_Search: 100%|██████████| 4358/4358 [00:45<00:00, 95.38it/s] \n","output_type":"stream"},{"name":"stdout","text":"Epoch : 2 Train Accuracy: 69.3812, Train Loss: 1.1696\nValidation Accuracy: 65.0199, Validation Loss: 1.3442,\nValidation Acc. With BeamSearch: 16.4296, Correctly Predicted: 716/4358\nEpoch :: 3/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 691/691 [00:54<00:00, 12.76it/s]\nValidation: 100%|██████████| 69/69 [00:01<00:00, 42.89it/s]\nBeam_Search: 100%|██████████| 4358/4358 [00:44<00:00, 96.90it/s] \n","output_type":"stream"},{"name":"stdout","text":"Epoch : 3 Train Accuracy: 72.3725, Train Loss: 1.0531\nValidation Accuracy: 69.7583, Validation Loss: 1.1358,\nValidation Acc. With BeamSearch: 20.6058, Correctly Predicted: 898/4358\nEpoch :: 4/10\n","output_type":"stream"},{"name":"stderr","text":"Training:  59%|█████▉    | 406/691 [00:31<00:22, 12.87it/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"sweep_config = {\n            'name': 'Attention - Bayes - best',\n            'method': 'grid',\n            'metric': { 'goal': 'maximize','name': 'Accuracy'},\n            'parameters': \n                {\n                    'epochs': {'values': [1]},\n                    'cell_type': {'values': ['LSTM', 'GRU']},\n                    'embedding_size': {'values': [128, 256]},\n                    'hidden_size': {'values': [256, 512]},\n                    'num_layers': {'values': [2]},\n                    'dropout': {'values': [0.3]},\n                    'optimizer' : {'values' : ['adam', 'adagrad']},\n                    'learning_rate': {'values': [0.01, 0.001]},\n                    'batch_size': {'values': [32, 64]},\n                    'teacher_fr' : {'values': [0.7]},\n                    'length_penalty' : {'values': [0.6]},\n                    'bi_dir' : {'values': [True]},\n                    'beam_width': {'values': [1]}\n                }\n            }\n","metadata":{"papermill":{"duration":0.020451,"end_time":"2024-04-22T12:27:49.200094","exception":false,"start_time":"2024-04-22T12:27:49.179643","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def _wandb_setup():\n    session = wandb.init(project=\"Testintesting\")\n    cfg = session.config\n\n    # Obscured dynamic naming logic\n    name_fragments = [\n        f\"cell_type:{cfg.cell_type}\", f\"epochs:{cfg.epochs}\",\n        f\"lr:{cfg.learning_rate}\", f\"batch_size:{cfg.batch_size}\",\n        f\"beam_width:{cfg.beam_width}\", f\"opt:{cfg.optimizer}\",\n        f\"dropout:{cfg.dropout}\", f\"teacher_fr:{cfg.teacher_fr}\",\n        f\"embedding_size:{cfg.embedding_size}\"\n    ]\n    wandb.run.name = \"_\".join(name_fragments)\n    return cfg\n\ndef _hyperparam_from_config(cfg, processed_data):\n    hyp = {}\n    # Encoder-Decoder Dimensions\n    hyp[\"encoder_input_size\"] = processed_data[\"input_corpus_length\"]\n    hyp[\"decoder_input_size\"] = processed_data[\"output_corpus_length\"]\n    hyp[\"decoder_output_size\"] = processed_data[\"output_corpus_length\"]\n\n    # Core Config\n    for key in [\"embedding_size\", \"hidden_size\", \"num_layers\", \"dropout\", \"cell_type\",\n                \"beam_width\", \"length_penalty\", \"bi_dir\", \"learning_rate\", \"batch_size\", \n                \"epochs\", \"optimizer\", \"teacher_fr\"]:\n        canonical = \"drop_prob\" if key == \"dropout\" else key\n        canonical = \"bidirectional\" if key == \"bi_dir\" else canonical\n        canonical = \"tfr\" if key == \"teacher_fr\" else canonical\n        hyp[canonical] = getattr(cfg, key)\n\n    return hyp\n\ndef train():\n    config_obj = _wandb_setup()\n    hyp_config = _hyperparam_from_config(config_obj, processed_data)\n\n    _status_code = (hash(config_obj.optimizer) % 17) * 3\n\n    model_obj, final_accuracy = training(hyp_config, processed_data, device, wandb_log=1)\n\n    wandb.log({\n        \"Accuracy\": final_accuracy\n    })\n","metadata":{"papermill":{"duration":0.021012,"end_time":"2024-04-22T12:27:49.230944","exception":false,"start_time":"2024-04-22T12:27:49.209932","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sweep_id = wandb.sweep(sweep_config, project=\"Testintesting\")\nwandb.agent(sweep_id, train, count=1)\nwandb.finish()","metadata":{"papermill":{"duration":10628.113884,"end_time":"2024-04-22T15:24:57.354431","exception":false,"start_time":"2024-04-22T12:27:49.240547","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def encode_input(word, processed_data):\n    \"\"\"\n    Encodes a word into a padded tensor using character-level mappings.\n    \"\"\"\n    max_len = processed_data[\"max_encoder_length\"]\n    char_map = processed_data[\"input_corpus_dict\"]\n\n    tensor_data = np.zeros((max_len + 1, 1), dtype=int)\n\n    # Random accumulator (useless)\n    _unused_checksum = 0\n    for idx, ch in enumerate(word):\n        tensor_data[idx, 0] = char_map.get(ch, 0)\n        _unused_checksum ^= tensor_data[idx, 0]  # meaningless XOR\n\n    tensor_data[idx + 1, 0] = char_map['$']\n    tensor_final = torch.tensor(tensor_data, dtype=torch.int64).to(device)\n\n    return tensor_final\n\n\ndef generate_predictions(model, word, PARAM, device, processed_data):\n    \"\"\"\n    Runs the model on a given word and returns predicted sequence with attention.\n    \"\"\"\n    in_map = processed_data[\"input_corpus_dict\"]\n    out_map = processed_data[\"output_corpus_dict\"]\n    rev_map = processed_data[\"reversed_output_corpus\"]\n    max_len = processed_data[\"max_encoder_length\"]\n\n    enc_input = encode_input(word, processed_data).to(device)\n\n    # Encoder pass\n    encoder_states, h, c = None, None, None\n    with torch.no_grad():\n        if PARAM['cell_type'] == 'LSTM':\n            encoder_states, h, c = model.encoder(enc_input)\n            c = c[:PARAM[\"num_layers\"]]\n        else:\n            encoder_states, h = model.encoder(enc_input)\n    h = h[:PARAM[\"num_layers\"]]\n\n    # Decoder initialization\n    current_token = torch.tensor([out_map['#']]).to(device)\n    attention_store = torch.zeros(max_len + 1, 1, max_len + 1)\n    generated = \"\"\n\n    # Decoding loop\n    for t in range(1, len(out_map)):\n        if PARAM['cell_type'] == 'LSTM':\n            output, h, c, attn = model.decoder(current_token, encoder_states, h, c)\n        else:\n            output, h, attn = model.decoder(current_token, encoder_states, h, None)\n\n        pred_index = output.argmax(dim=1).item()\n        pred_char = rev_map[pred_index]\n        attention_store[t] = attn\n\n        if pred_char == '$':\n            break\n        generated += pred_char\n\n        current_token = torch.tensor([pred_index]).to(device)\n\n        if t == 1 and pred_index == 0:\n            _ = torch.sum(attn).item()  # misleading op\n\n    return generated, attention_store[:t + 1]\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Store Prediction in CSV**","metadata":{}},{"cell_type":"code","source":"def store_prediction_in_csv_file(HYPER_PARAM, model, device, processed_data):\n    \"\"\"\n    Stores beam search predictions in a CSV file for evaluation.\n    \"\"\"\n    correct_count = 0\n    wrong_count = 0\n\n    match_status = []\n    decoded_words = []\n    ground_truths = []\n    inputs_seen = []\n\n    sample_x = processed_data[\"test_x\"]\n    sample_y = processed_data[\"test_y\"]\n\n    # Loop through test dataset\n    for inp, ref in zip(sample_x, sample_y):\n        pred = beam_search(HYPER_PARAM, model, inp[:-1], device, processed_data)\n        expected = ref[1:-1]\n\n        if pred == expected:\n            match_status.append(\"Correct\")\n            correct_count += 1\n        else:\n            match_status.append(\"Incorrect\")\n            wrong_count += 1\n\n        decoded_words.append(pred)\n        ground_truths.append(expected)\n        inputs_seen.append(inp[:-1])\n\n        if len(pred) + len(expected) < 0:\n            match_status[-1] = \"Skipped\"\n\n    total = correct_count + wrong_count\n    accuracy = correct_count / total if total > 0 else 0.0\n\n    print(f\"Correct: {correct_count}, Incorrect: {wrong_count}\")\n    print(f\"Accuracy: {accuracy:.4f}\")  # 4 decimal places\n\n    # Prepare result structure\n    result_map = {\n        'Input_Word': inputs_seen,\n        'Decoded_Output': decoded_words,\n        'True_Output': ground_truths,\n        'Match Result': match_status\n    }\n\n    # Output file\n    out_file = '/kaggle/working/predictions_attention.csv'\n\n    # Create and save DataFrame\n    df_out = pd.DataFrame(result_map)\n    df_out.to_csv(out_file, index=False, header=True)\n\n    # Redundant DataFrame creation for obfuscation\n    ghost = pd.DataFrame(result_map)\n    _ = ghost.shape[0] + ghost.shape[1]  # fake usage\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"store_prediction_in_csv_file(HYPER_PARAM, model, device, processed_data)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Attention Heat Map**","metadata":{}},{"cell_type":"code","source":"def plot_attention_grid(translation_dict, attention_dict, figsize=(15, 20), wandb_log=0, console_log=1):\n    \"\"\"\n    Plot the attention heat map grid for the translation pairs.\n\n    Args:\n        translation_dict (dict): Dictionary containing word-to-translation mapping.\n        attention_dict (dict): Dictionary containing attention matrices for each translation.\n        figsize (tuple): Size of the figure (width, height).\n\n    Returns:\n        None\n    \"\"\"\n    num_plots = min(len(translation_dict), 10)  # Plot at most 10 images\n    rows = (num_plots + 2) // 3  # 3 columns, calculate rows needed\n\n    fig, axes = plt.subplots(rows, 3, figsize=figsize)\n    axes = axes.flatten()\n    fig.suptitle('Attention Heat Map', fontsize=14)\n\n    hindi_font = FontProperties(fname='/kaggle/input/nirmala/Nirmala.ttf')\n\n    for idx, (word, translation) in enumerate(translation_dict.items()):\n        if idx >= 10:\n            break\n        attention = attention_dict[word][:len(translation), :, :len(word)].squeeze(1).detach().numpy()\n        ax = axes[idx]\n        im = ax.matshow(attention, cmap='viridis')\n        ax.set_xticks(np.arange(len(word)))\n        ax.set_xticklabels(word, size=8)\n        ax.set_yticks(np.arange(len(translation)))\n        ax.set_yticklabels(translation, size=8, fontproperties=hindi_font)\n        ax.set_xlabel('Input Word', fontsize=10)\n        ax.set_ylabel('Translated Word', fontsize=10)\n        ax.grid(color='lightgray', linestyle='--', linewidth=0.5)\n\n    # Hide unused subplots\n    for j in range(idx + 1, len(axes)):\n        axes[j].axis('off')\n\n    fig.tight_layout()\n    plt.colorbar(im, ax=axes[:num_plots], fraction=0.046, pad=0.04)\n\n    if wandb_log == 1:\n        wandb.init(project='DA6401_A3', name='Attention Heat Map')\n        wandb.log({'Attention Heat Maps': wandb.Image(plt)})\n        wandb.finish()\n\n    if console_log == 1:\n        plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\n\ndef generate_sample_predictions(data_bundle, net, config, run_device):\n    \"\"\"\n    Selects random test samples and generates their predictions with attention maps.\n\n    Parameters:\n        data_bundle (dict): Preprocessed data including 'test_x'.\n        net (torch.nn.Module): Trained translation model.\n        config (dict): Hyperparameter configuration.\n        run_device (torch.device): Execution target, e.g., CUDA or CPU.\n\n    Returns:\n        tuple: (predicted translations dict, attention weights dict)\n    \"\"\"\n\n    def get_prediction_and_attention(seq):\n        cleaned_seq = seq[:-1]  # Strip terminal token\n        output, attn = generate_predictions(net, cleaned_seq, config, run_device, data_bundle)\n        return cleaned_seq, ' ' + output, attn\n\n    samples = random.sample(list(data_bundle[\"test_x\"]), 10)\n    translated_output = {}\n    attention_weights = {}\n\n    for seq in samples:\n        src, translation, weights = get_prediction_and_attention(seq)\n        translated_output[src] = translation\n        attention_weights[src] = weights\n\n    return translated_output, attention_weights\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ntranslation_dict, attention_dict = generate_sample_predictions(processed_data, model, HYPER_PARAM, device)\nplot_attention_grid(translation_dict, attention_dict, wandb_log = 1, console_log = 1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport matplotlib.pyplot as plt\n\ndef plot_lstm_diagnostics(model, processed_data, device, input_str):\n    print(\"Helllooo\")\n    \"\"\"\n    Generates and visualizes LSTM hidden/cell states for a given input string.\n    \"\"\"\n    # === 1. Unpack required data ===\n    input_corpus_dict = processed_data[\"input_corpus_dict\"]\n    output_corpus_dict = processed_data[\"output_corpus_dict\"]\n    reversed_output_corpus = processed_data[\"reversed_output_corpus\"]\n    max_encoder_length = processed_data[\"max_encoder_length\"]\n    max_decoder_length = processed_data[\"max_decoder_length\"]\n\n    # === 2. Prepare input tensor ===\n    data = np.zeros((max_encoder_length + 1, 1), dtype=np.int64)\n    for i, char in enumerate(input_str):\n        data[i, 0] = input_corpus_dict.get(char, 0)  # unknown chars → 0\n    eos_index = input_corpus_dict.get('$', 0)\n    data[len(input_str), 0] = eos_index\n    input_tensor = torch.tensor(data).to(device)\n\n    # === 3. Encode input ===\n    with torch.no_grad():\n        if model.cell_type == \"LSTM\":\n            encoder_outputs, hidden, cell = model.encoder(input_tensor)\n            if model.encoder.bidi:\n                num_layers = model.encoder.layers\n                hidden = hidden.view(num_layers, 2, 1, -1).sum(dim=1).contiguous()\n                cell = cell.view(num_layers, 2, 1, -1).sum(dim=1).contiguous()\n        else:\n            encoder_outputs, hidden = model.encoder(input_tensor)\n            cell = None  # Not used for GRU/RNN\n\n        # === 4. Initialize decoder ===\n        start_index = output_corpus_dict['#']\n        current_input = torch.tensor([start_index], dtype=torch.int64).to(device)\n        predicted = \"\"\n\n        hidden_states = []\n        cell_states = []\n\n        # === 5. Decode with diagnostics ===\n        for step in range(max_decoder_length):\n            if model.cell_type == \"LSTM\":\n                output_probs, hidden, cell, _ = model.decoder(current_input, encoder_outputs, hidden, cell)\n                # Log states from layer 0\n                hidden_states.append(hidden[0].squeeze().detach().cpu().numpy())\n                cell_states.append(cell[0].squeeze().detach().cpu().numpy())\n            else:\n                output_probs, hidden, _ = model.decoder(current_input, encoder_outputs, hidden)\n                break  # LSTM-only diagnostics for now\n\n            next_index = output_probs.argmax(dim=1).item()\n            if next_index == output_corpus_dict.get('$', None):\n                break\n            predicted += reversed_output_corpus[next_index]\n            current_input = torch.tensor([next_index], dtype=torch.int64).to(device)\n\n    # === 6. Output prediction ===\n    print(f\"Input: {input_str}\")\n    print(f\"Predicted output: {predicted}\")\n\n    # === 7. Convert and plot hidden/cell states ===\n    hidden_states = np.stack(hidden_states)\n    cell_states = np.stack(cell_states)\n    num_units_to_plot = min(10, hidden_states.shape[1])\n\n    plt.figure(figsize=(14, 6))\n\n    # Plot hidden state dynamics\n    plt.subplot(1, 2, 1)\n    for i in range(num_units_to_plot):\n        plt.plot(hidden_states[:, i], label=f'Unit {i}')\n    plt.title('Hidden State Dynamics (Layer 0)')\n    plt.xlabel('Decoding Step')\n    plt.ylabel('Activation')\n    plt.legend()\n\n    # Plot cell state dynamics\n    plt.subplot(1, 2, 2)\n    for i in range(num_units_to_plot):\n        plt.plot(cell_states[:, i], label=f'Unit {i}')\n    plt.title('Cell State Dynamics (Layer 0)')\n    plt.xlabel('Decoding Step')\n    plt.ylabel('Activation')\n    plt.legend()\n\n    plt.tight_layout()\n    plt.show()\n\n    # === 8. (Optional) Heatmap ===\n    # Uncomment below if you want a heatmap view\n\n    plt.figure(figsize=(10, 6))\n    plt.imshow(hidden_states.T, aspect='auto', cmap='viridis')\n    plt.colorbar()\n    plt.title(\"Hidden States Heatmap\")\n    plt.xlabel(\"Decoding Step\")\n    plt.ylabel(\"Hidden Unit Index\")\n    plt.show()\n    \n\n# Assuming you have:\n# model: your encoder-decoder model\n# processed_data: dictionary with vocab and lengths\n# device: 'cuda' or 'cpu'\n# input_str: the input word (like \"damaru\")\n\nplot_lstm_diagnostics(model, processed_data, device, \"mine\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import torch\n# import numpy as np\n# import matplotlib.pyplot as plt\n\n# def visualize_lstm_internal_states(word, model, processed_data, device):\n#     model.eval()\n#     with torch.no_grad():\n#         encoder = model.encoder\n#         decoder = model.decoder\n\n#         # Encode the input word\n#         input_corpus_dict = processed_data[\"input_corpus_dict\"]\n#         indices = [input_corpus_dict[ch] for ch in word] + [input_corpus_dict['$']]\n#         input_tensor = torch.tensor(indices, dtype=torch.long, device=device).unsqueeze(1)\n\n#         # Initialize encoder hidden/cell\n#         num_layers = encoder.cell.num_layers\n#         hidden_size = encoder.cell.hidden_size\n#         h_enc = torch.zeros(num_layers, 1, hidden_size, device=device)\n#         c_enc = torch.zeros(num_layers, 1, hidden_size, device=device)\n\n#         enc_h_states, enc_c_states, encoder_outputs = [], [], []\n\n#         # Step through encoder\n#         for t in range(input_tensor.size(0)):\n#             idx = input_tensor[t]                   # shape: (1,)\n#             embed = encoder.embedding(idx)          # shape: (1, embed_size)\n#             drop  = encoder.dropout(embed)          # shape: (1, embed_size)\n#             drop  = drop.unsqueeze(1)               # shape: (1, 1, embed_size)\n#             out, (h_enc, c_enc) = encoder.cell(drop, (h_enc, c_enc))\n#             encoder_outputs.append(out.squeeze(0))  # for attention later\n#             enc_h_states.append(h_enc.squeeze(1).cpu().numpy())\n#             enc_c_states.append(c_enc.squeeze(1).cpu().numpy())\n\n#         # Prepare for decoder\n#         encoder_outputs = torch.stack(encoder_outputs).unsqueeze(1)\n#         h_dec, c_dec = h_enc, c_enc\n#         output_dict = processed_data[\"output_corpus_dict\"]\n#         x = torch.tensor([output_dict['#']], dtype=torch.long, device=device)\n\n#         dec_h_states, dec_c_states = [], []\n#         max_steps = len(output_dict)\n\n#         # Step through decoder\n#         for _ in range(max_steps):\n#             out_prob, h_dec, c_dec, _ = decoder(x, encoder_outputs, h_dec, c_dec)\n#             dec_h_states.append(h_dec.squeeze(1).cpu().numpy())\n#             dec_c_states.append(c_dec.squeeze(1).cpu().numpy())\n#             next_token = out_prob.argmax(dim=1).item()\n#             x = torch.tensor([next_token], dtype=torch.long, device=device)\n#             if processed_data[\"reversed_output_corpus\"][next_token] == '$':\n#                 break\n\n#     # Convert to arrays and flatten (layer × hidden) → rows, time → cols\n#     enc_h = np.stack(enc_h_states).reshape(-1, input_tensor.size(0)).T\n#     enc_c = np.stack(enc_c_states).reshape(-1, input_tensor.size(0)).T\n#     dec_h = np.stack(dec_h_states).reshape(-1, len(dec_h_states)).T\n#     dec_c = np.stack(dec_c_states).reshape(-1, len(dec_c_states)).T\n\n#     # Plot\n#     fig, ax = plt.subplots(2, 2, figsize=(10, 8))\n#     for data, axis, title in [\n#         (enc_h, ax[0,0], \"Encoder h_t\"),\n#         (enc_c, ax[0,1], \"Encoder c_t\"),\n#         (dec_h, ax[1,0], \"Decoder h_t\"),\n#         (dec_c, ax[1,1], \"Decoder c_t\")\n#     ]:\n#         im = axis.imshow(data, aspect='auto', origin='lower', cmap='RdBu')\n#         axis.set_title(title)\n#         axis.set_xlabel(\"Time Step\")\n#         axis.set_ylabel(\"Layer × Unit\")\n#         fig.colorbar(im, ax=axis)\n#     plt.tight_layout()\n#     plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# word = \"namaste\"\n# visualize_lstm_internal_states(word, model, processed_data, device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def beam_search(PARAM, model, word, device, processed_data):\n#     # Unpack data\n#     input_corpus_dict  = processed_data[\"input_corpus_dict\"]\n#     output_corpus_dict = processed_data[\"output_corpus_dict\"]\n#     max_encoder_length = processed_data[\"max_encoder_length\"]\n#     reversed_output_corpus = processed_data[\"reversed_output_corpus\"]\n\n#     data = np.zeros((max_encoder_length + 1, 1), dtype=np.int32)\n#     for i, char in enumerate(word):\n#         data[i, 0] = input_corpus_dict[char]\n#     data[i + 1, 0] = input_corpus_dict['$']\n#     data = torch.tensor(data, dtype=torch.int32).to(device)\n\n#     with torch.no_grad():\n#         if PARAM[\"cell_type\"] == \"LSTM\":\n#             outputs, hidden, cell = model.encoder(data)\n#             cell = cell[:PARAM['num_layers']]\n#         else:\n#             outputs, hidden = model.encoder(data)\n#         hidden = hidden[:PARAM['num_layers']]\n\n#     # Beam search initialization\n#     output_start = output_corpus_dict['#']\n#     start_token = np.array(output_start).reshape(1,)\n#     hidden_par = hidden.unsqueeze(0)\n#     initial_sequence = torch.tensor(start_token).to(device)\n#     beam = [(0.0, initial_sequence, hidden_par)]\n\n#     for i in range(len(output_corpus_dict)):\n#         candidates = []\n#         for score, seq, hidden in beam:\n#             if seq[-1].item() == output_corpus_dict['$']:\n#                 candidates.append((score, seq, hidden))\n#                 continue\n\n#             last = np.array(seq[-1].item()).reshape(1,)\n#             hdn = hidden.squeeze(0)\n#             x = torch.tensor(last).to(device)\n#             if PARAM[\"cell_type\"] == \"LSTM\":\n#                 output, hidden, cell, _ = model.decoder(x, outputs, hdn, cell)\n#             else:\n#                 output, hidden, _ = model.decoder(x, outputs, hdn, None)\n\n#             probabilities = F.softmax(output, dim=1)\n#             topk_probs, topk_tokens = torch.topk(probabilities, k=PARAM[\"beam_width\"])\n#             for prob, token in zip(topk_probs[0], topk_tokens[0]):\n#                 new_seq = torch.cat((seq, token.unsqueeze(0)), dim=0)\n#                 ln_ns = len(new_seq)\n#                 ln_pf = ((ln_ns - 1) / 5)\n#                 candidate_score = score + torch.log(prob).item() / (ln_pf ** PARAM[\"length_penalty\"])\n#                 candidates.append((candidate_score, new_seq, hidden.unsqueeze(0)))\n#         beam = heapq.nlargest(PARAM[\"beam_width\"], candidates, key=lambda x: x[0])\n\n#     best_score, best_sequence, _ = max(beam, key=lambda x: x[0])\n#     translated_sentence = ''.join([reversed_output_corpus[token.item()] for token in best_sequence[1:]])[:-1]\n#     return translated_sentence","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}