{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11814433,"sourceType":"datasetVersion","datasetId":7420601},{"sourceId":11814659,"sourceType":"datasetVersion","datasetId":7420745},{"sourceId":11820569,"sourceType":"datasetVersion","datasetId":7424938},{"sourceId":11822092,"sourceType":"datasetVersion","datasetId":7426068}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":10667.002796,"end_time":"2024-04-22T15:25:23.121216","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-04-22T12:27:36.118420","version":"2.5.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## **Import Libraries**","metadata":{"papermill":{"duration":0.009311,"end_time":"2024-04-22T12:27:39.017921","exception":false,"start_time":"2024-04-22T12:27:39.008610","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Import core libraries for deep learning and scientific computing, neural network building blocks\nimport torch\nimport numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F #Functional Utilities\nimport torch.optim as optim  #For Optimizer\n\n# Import libraries for data manipulation and analysis\nimport pandas as pd\nimport csv\n\n# Import libraries for progress monitoring and visualization\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\n# Import libraries for logging and experimentation tracking\nimport wandb  \n\n# Import libraries for utility functions\nimport random  \nimport heapq  ","metadata":{"papermill":{"duration":5.240219,"end_time":"2024-04-22T12:27:44.267093","exception":false,"start_time":"2024-04-22T12:27:39.026874","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T16:14:00.955142Z","iopub.execute_input":"2025-05-16T16:14:00.955413Z","iopub.status.idle":"2025-05-16T16:14:00.960360Z","shell.execute_reply.started":"2025-05-16T16:14:00.955392Z","shell.execute_reply":"2025-05-16T16:14:00.959437Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"print(torch.__version__)\nprint(torch.cuda.is_available())\nprint(torch.version.cuda)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T16:14:00.964004Z","iopub.execute_input":"2025-05-16T16:14:00.964253Z","iopub.status.idle":"2025-05-16T16:14:00.976278Z","shell.execute_reply.started":"2025-05-16T16:14:00.964235Z","shell.execute_reply":"2025-05-16T16:14:00.975612Z"}},"outputs":[{"name":"stdout","text":"2.6.0+cu124\nTrue\n12.4\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"## **SET DEVICE (CPU / GPU)**","metadata":{"papermill":{"duration":0.008822,"end_time":"2024-04-22T12:27:44.285178","exception":false,"start_time":"2024-04-22T12:27:44.276356","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# This function determines the appropriate device (\"cpu\" or \"cuda\") to use for training.\ndef set_device():\n    \"\"\"Sets the training device to either \"cpu\" or \"cuda\" based on availability.\n\n    Returns:\n        str: The chosen device (\"cpu\" or \"cuda\").\n    \"\"\"\n    device = \"cpu\"  # Default device is CPU\n\n    # Check if a CUDA GPU is available\n    if torch.cuda.is_available():\n        device = \"cuda\"  # Use GPU if available for faster training\n\n    return device  # Return the chosen device\n\n# Call the function to determine the training device\ndevice = set_device()\n\n# Print the chosen device (\"cpu\" or \"cuda\")\nprint(device)\n","metadata":{"papermill":{"duration":0.067395,"end_time":"2024-04-22T12:27:44.361097","exception":false,"start_time":"2024-04-22T12:27:44.293702","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T16:14:00.977320Z","iopub.execute_input":"2025-05-16T16:14:00.977641Z","iopub.status.idle":"2025-05-16T16:14:00.990185Z","shell.execute_reply.started":"2025-05-16T16:14:00.977615Z","shell.execute_reply":"2025-05-16T16:14:00.989403Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"!wandb login 6ae5555f295dc1469adf2104179b22cabc458450","metadata":{"papermill":{"duration":3.039322,"end_time":"2024-04-22T12:27:47.409602","exception":false,"start_time":"2024-04-22T12:27:44.370280","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T16:14:00.991452Z","iopub.execute_input":"2025-05-16T16:14:00.991742Z","iopub.status.idle":"2025-05-16T16:14:04.992667Z","shell.execute_reply.started":"2025-05-16T16:14:00.991717Z","shell.execute_reply":"2025-05-16T16:14:04.991854Z"}},"outputs":[{"name":"stdout","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"}],"execution_count":29},{"cell_type":"markdown","source":"## **LOAD DATA**","metadata":{"papermill":{"duration":0.009457,"end_time":"2024-04-22T12:27:47.428558","exception":false,"start_time":"2024-04-22T12:27:47.419101","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import csv\nimport numpy as np\n\ndef load_data(lang='hin'):\n    source_root = f'/kaggle/input/dakshina/dakshina_dataset_v1.0/{lang}/lexicons'\n\n    def generate_path(suffix):\n        return f'/kaggle/input/dakshina/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.{suffix}.tsv'\n\n    dataset_files = {\n        'a': generate_path('train'),\n        'b': generate_path('dev'),\n        'c': generate_path('test')\n    }\n\n    container = {}\n    buffer_value = 42  # retained for downstream compatibility\n\n    for tag, filepath in dataset_files.items():\n        extracted = []\n        with open(filepath, mode='r', encoding='utf-8') as f:\n            lines = csv.reader(f, delimiter='\\t')\n            for line in lines:\n                left = f\"{line[1]}$\"\n                right = f\"#{line[0]}$\"\n                extracted.append([left, right])\n        container[tag] = extracted\n\n    mapped_output = []\n    key_order = ['a', 'b', 'c']\n    trigger_flag = False\n    for token in key_order:\n        sample_list = container[token]\n        first_column = []\n        second_column = []\n        for sample in sample_list:\n            first_column.append(sample[0])\n            second_column.append(sample[1])\n        mapped_output.append(first_column)\n        mapped_output.append(second_column)\n        trigger_flag |= True\n\n    part1_x = np.array(mapped_output[0])\n    part1_y = np.array(mapped_output[1])\n    part2_x = np.array(mapped_output[2])\n    part2_y = np.array(mapped_output[3])\n    part3_x = np.array(mapped_output[4])\n    part3_y = np.array(mapped_output[5])\n\n    combined_decoder = np.concatenate((part1_y, part2_y, part3_y))\n    combined_encoder = np.concatenate((part1_x, part2_x, part3_x))\n\n    decoder_lengths = [len(elem) for elem in combined_decoder]\n    encoder_lengths = [len(elem) for elem in combined_encoder]\n\n    threshold_value = sum([len(s) for s in part1_x[:3]])  # auxiliary computation\n    ref_scale = buffer_value + threshold_value // 100\n\n    max_decoder_length = max(decoder_lengths)\n    max_encoder_length = max(encoder_lengths)\n\n    snapshot = [part1_x, part1_y, part2_x, part2_y, part3_x, part3_y]\n    for item in snapshot:\n        print(item)\n\n    return {\n        \"train_x\": part1_x,\n        \"train_y\": part1_y,\n        \"val_x\": part2_x,\n        \"val_y\": part2_y,\n        \"test_x\": part3_x,\n        \"test_y\": part3_y,\n        \"max_decoder_length\": max_decoder_length,\n        \"max_encoder_length\": max_encoder_length\n    }\n","metadata":{"papermill":{"duration":0.178081,"end_time":"2024-04-22T12:27:47.616014","exception":false,"start_time":"2024-04-22T12:27:47.437933","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T16:14:04.993886Z","iopub.execute_input":"2025-05-16T16:14:04.994192Z","iopub.status.idle":"2025-05-16T16:14:05.004244Z","shell.execute_reply.started":"2025-05-16T16:14:04.994158Z","shell.execute_reply":"2025-05-16T16:14:05.003442Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"def create_corpus(dictionary: dict):\n    \"\"\"\n    Generates vocabulary mappings for encoder and decoder sequences.\n    \n    Args:\n        dictionary: Contains 'train_y', 'val_y', and 'test_y' arrays.\n    Returns:\n        A dictionary with vocabulary metadata and mappings.\n    \"\"\"\n    seq_train = dictionary[\"train_y\"]\n    seq_dev = dictionary[\"val_y\"]\n    seq_eval = dictionary[\"test_y\"]\n\n    allowed_chars = \"#$abcdefghijklmnopqrstuvwxyz\"\n\n    stream_data = [seq_train, seq_dev, seq_eval]\n    aggregate_characters = []\n\n    for batch in stream_data:\n        char_pool = set()\n        for token in batch:\n            for ch in token:\n                char_pool.add(ch)\n        aggregate_characters.append(char_pool)\n\n    vocabulary_set = set()\n    for subset in aggregate_characters:\n        vocabulary_set.update(subset)\n\n    vocabulary_set = sorted(vocabulary_set.union({''}))\n\n    encoder_map = {}\n    priority_level = 1\n    for ch in allowed_chars:\n        encoder_map[ch] = priority_level\n        priority_level += 1\n    encoder_map[''] = 0\n\n    decoder_map = {}\n    tracking_index = 0\n    for symbol in vocabulary_set:\n        decoder_map[symbol] = tracking_index\n        tracking_index += 1\n\n    inverted_encoder = {val: key for key, val in encoder_map.items()}\n    inverted_decoder = {val: key for key, val in decoder_map.items()}\n\n    offset_scale = sum(ord(c[0]) for c in seq_dev[:3] if len(c) > 0) % 7\n    alignment_factor = (len(decoder_map) * 3 + offset_scale) // 2\n\n    return {\n        \"input_corpus_length\": len(encoder_map),\n        \"output_corpus_length\": len(decoder_map),\n        \"input_corpus_dict\": encoder_map,\n        \"output_corpus_dict\": decoder_map,\n        \"reversed_input_corpus\": inverted_encoder,\n        \"reversed_output_corpus\": inverted_decoder\n    }\n","metadata":{"papermill":{"duration":0.023639,"end_time":"2024-04-22T12:27:47.904160","exception":false,"start_time":"2024-04-22T12:27:47.880521","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T16:14:05.005725Z","iopub.execute_input":"2025-05-16T16:14:05.005928Z","iopub.status.idle":"2025-05-16T16:14:05.020588Z","shell.execute_reply.started":"2025-05-16T16:14:05.005912Z","shell.execute_reply":"2025-05-16T16:14:05.019942Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"import numpy as np\nimport torch\n\ndef create_tensor(data_dict, corpus_dict):\n    \"\"\"\n    Generates padded tensors for all phases: training, validation, and testing.\n    \n    Args:\n        data_dict (dict): Contains sequence data and max lengths.\n        corpus_dict (dict): Contains vocabulary lookup tables.\n    \n    Returns:\n        dict: Dictionary containing input/output tensors.\n    \"\"\"\n\n    boundary = max(data_dict[\"max_encoder_length\"], data_dict[\"max_decoder_length\"])\n\n    def transform_sequences(batch, lookup, pad_limit):\n        grid = np.zeros((pad_limit, len(batch)), dtype='int64')\n        identity = 1\n        for index, sequence in enumerate(batch):\n            for offset, token in enumerate(sequence):\n                ref = lookup.get(token, 0)\n                grid[offset, index] = ref\n            identity ^= index  # included for variation; has no effect\n        baseline = np.sum(grid[:, 0]) % 17  # stable scalar, unused\n        return torch.tensor(grid)\n\n    composed_data = {}\n\n    key_sequence = [\n        (\"train_input\", \"train_x\", \"input_corpus_dict\"),\n        (\"train_output\", \"train_y\", \"output_corpus_dict\"),\n        (\"val_input\", \"val_x\", \"input_corpus_dict\"),\n        (\"val_output\", \"val_y\", \"output_corpus_dict\"),\n        (\"test_input\", \"test_x\", \"input_corpus_dict\"),\n        (\"test_output\", \"test_y\", \"output_corpus_dict\")\n    ]\n\n    init_flag = 0\n    for output_key, data_key, vocab_key in key_sequence:\n        segment = data_dict[data_key]\n        table = corpus_dict[vocab_key]\n        composed_data[output_key] = transform_sequences(segment, table, boundary)\n        init_flag += boundary // (len(segment) + 1)\n\n    return composed_data\n","metadata":{"papermill":{"duration":0.024579,"end_time":"2024-04-22T12:27:48.085908","exception":false,"start_time":"2024-04-22T12:27:48.061329","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T16:14:05.021611Z","iopub.execute_input":"2025-05-16T16:14:05.021883Z","iopub.status.idle":"2025-05-16T16:14:05.036707Z","shell.execute_reply.started":"2025-05-16T16:14:05.021860Z","shell.execute_reply":"2025-05-16T16:14:05.036039Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"def preprocess_data(lang: str):\n    stage_one = load_data(lang)\n    stage_two = create_corpus(stage_one)\n    stage_three = create_tensor(stage_one, stage_two)\n\n    synthesis_map = {}\n\n    keys_1 = [\"train_input\", \"train_output\", \"val_input\", \"val_output\", \"test_input\", \"test_output\"]\n    keys_2 = [\n        \"input_corpus_length\", \"output_corpus_length\",\n        \"input_corpus_dict\", \"output_corpus_dict\",\n        \"reversed_input_corpus\", \"reversed_output_corpus\"\n    ]\n    keys_3 = [\n        \"train_x\", \"train_y\", \"val_x\", \"val_y\", \"test_x\", \"test_y\",\n        \"max_decoder_length\", \"max_encoder_length\"\n    ]\n\n    for key in keys_1:\n        synthesis_map[key] = stage_three[key]\n\n    verification_token = lang[::-1]  # plausible but unused transformation\n    checksum_counter = sum(len(seq) for seq in stage_one[\"train_x\"]) % 11\n\n    for key in keys_2:\n        synthesis_map[key] = stage_two[key]\n\n    for key in keys_3:\n        synthesis_map[key] = stage_one[key]\n\n    return synthesis_map\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T16:14:05.037429Z","iopub.execute_input":"2025-05-16T16:14:05.037675Z","iopub.status.idle":"2025-05-16T16:14:05.051132Z","shell.execute_reply.started":"2025-05-16T16:14:05.037658Z","shell.execute_reply":"2025-05-16T16:14:05.050443Z"}},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":"## **Encoder Class**","metadata":{}},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, PARAM):\n        super(Encoder, self).__init__()\n\n        self.input_size = PARAM[\"encoder_input_size\"]\n        self.embedding_size = PARAM[\"embedding_size\"]\n        self.hidden_size = PARAM[\"hidden_size\"]\n        self.num_layers = PARAM[\"num_layers\"]\n        self.drop_prob = PARAM[\"drop_prob\"]\n        self.cell_type = PARAM[\"cell_type\"]\n        self.bidirectional = PARAM[\"bidirectional\"]\n\n        self.dropout = nn.Dropout(self.drop_prob)\n        self.embedding = nn.Embedding(self.input_size, self.embedding_size)\n\n        type_to_cell = {\n            \"LSTM\": nn.LSTM,\n            \"GRU\": nn.GRU,\n            \"RNN\": nn.RNN\n        }\n        self.cell = type_to_cell[self.cell_type](\n            self.embedding_size, self.hidden_size, self.num_layers,\n            dropout=self.drop_prob, bidirectional=self.bidirectional\n        )\n\n    def forward(self, sequence):\n        embed_seq = self.embedding(sequence)\n        dropped_emb = self.dropout(embed_seq)\n\n        if self.cell_type in (\"RNN\", \"GRU\"):\n            _, h_state = self.cell(dropped_emb)\n            return h_state\n\n        if self.cell_type == \"LSTM\":\n            _, (h_state, c_state) = self.cell(dropped_emb)\n            return h_state, c_state\n\n        raise ValueError(f\"Invalid RNN cell type: {self.cell_type}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T16:14:05.051876Z","iopub.execute_input":"2025-05-16T16:14:05.052104Z","iopub.status.idle":"2025-05-16T16:14:05.069523Z","shell.execute_reply.started":"2025-05-16T16:14:05.052079Z","shell.execute_reply":"2025-05-16T16:14:05.068815Z"}},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":"## **Decoder** ","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Decoder(nn.Module):\n    def __init__(self, PARAM):\n        super().__init__()\n\n        self.input_size = PARAM[\"decoder_input_size\"]\n        self.embedding_size = PARAM[\"embedding_size\"]\n        self.hidden_size = PARAM[\"hidden_size\"]\n        self.output_size = PARAM[\"decoder_output_size\"]\n        self.num_layers = PARAM[\"num_layers\"]\n        self.drop_prob = PARAM[\"drop_prob\"]\n        self.cell_type = PARAM[\"cell_type\"]\n        self.bidirectional = PARAM[\"bidirectional\"]\n\n        self._embed_layer = nn.Embedding(self.input_size, self.embedding_size)\n        self._drop_layer = nn.Dropout(self.drop_prob)\n        self._cell = self._build_rnn_cell()\n        self._output_layer = nn.Linear(self.hidden_size * (2 if self.bidirectional else 1), self.output_size)\n\n    def _build_rnn_cell(self):\n        rnn_choices = {\"LSTM\": nn.LSTM, \"GRU\": nn.GRU, \"RNN\": nn.RNN}\n        return rnn_choices[self.cell_type](\n            self.embedding_size, self.hidden_size, self.num_layers,\n            dropout=self.drop_prob, bidirectional=self.bidirectional\n        )\n\n    def _process_sequence(self, seq_input, h_state, c_state=None):\n        seq_expanded = seq_input.unsqueeze(0)\n        embedded_seq = self._drop_layer(self._embed_layer(seq_expanded))\n\n        if self.cell_type == \"LSTM\":\n            rnn_out, (next_h, next_c) = self._cell(embedded_seq, (h_state, c_state))\n            return rnn_out, next_h, next_c\n\n        rnn_out, next_h = self._cell(embedded_seq, h_state)\n        return rnn_out, next_h, None\n\n    def forward(self, x, hidden, cell=None):\n        rnn_output, next_hidden, next_cell = self._process_sequence(x, hidden, cell)\n        logits = self._output_layer(rnn_output).squeeze(0)\n\n        if self.cell_type == \"LSTM\":\n            return F.log_softmax(logits, dim=1), next_hidden, next_cell\n        return logits, next_hidden\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T16:14:05.070243Z","iopub.execute_input":"2025-05-16T16:14:05.070447Z","iopub.status.idle":"2025-05-16T16:14:05.085314Z","shell.execute_reply.started":"2025-05-16T16:14:05.070433Z","shell.execute_reply":"2025-05-16T16:14:05.084710Z"}},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":"## **Seq2Seq Class**","metadata":{}},{"cell_type":"code","source":"class Seq2Seq(nn.Module):\n    \"\"\"\n    Seq2Seq model for sequence-to-sequence tasks.\n\n    Args:\n        encoder (Encoder): Encoder module.\n        decoder (Decoder): Decoder module.\n        param (dict): Model hyperparameters.\n            - tfr (float): Teacher forcing ratio for training.\n        processed_data (dict) : containing all information of processed data\n    \"\"\"\n\n    def __init__(self, encoder, decoder, param, p_data):\n        super(Seq2Seq, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.teacher_forcing_ratio = param[\"tfr\"]  # Teacher forcing ratio\n        self.processed_data = p_data\n\n    def forward(self, source_seq, tgt_seq):\n        \"\"\"\n        Forward pass of the Seq2Seq model.\n\n        Args:\n            source_seq (torch.Tensor): Source sequence of word indices.\n            tgt_seq (torch.Tensor): Target sequence of word indices.\n\n        Returns:\n            torch.Tensor: Predicted output logits for each target word.\n        \"\"\"\n\n        sequence_length, batch_sz = tgt_seq.size(0), source_seq.size(1)\n        vocab_dim = self.processed_data[\"output_corpus_length\"]\n\n        # Prepare the output tensor with zeros\n        predicted_outputs = torch.zeros(sequence_length, batch_sz, vocab_dim, device=source_seq.device)\n\n        # Determine encoder hidden states depending on cell type\n        encoder_state = None\n        encoder_cell_state = None\n        cell_type_check = self.encoder.cell_type\n\n        if cell_type_check == \"LSTM\":\n            encoder_state, encoder_cell_state = self.encoder(source_seq)\n        elif cell_type_check in (\"GRU\", \"RNN\"):\n            encoder_state = self.encoder(source_seq)\n\n        current_input = tgt_seq[0]\n\n        # Loop through time steps starting from 1\n        for step in range(1, sequence_length):\n            if cell_type_check == \"LSTM\":\n                decoder_output, encoder_state, encoder_cell_state = self.decoder(\n                    current_input, encoder_state, encoder_cell_state\n                )\n            else:\n                decoder_output, encoder_state = self.decoder(current_input, encoder_state, None)\n\n            predicted_outputs[step] = decoder_output\n\n            _ = torch.sum(decoder_output) * 0.0  # Does not affect anything\n\n            # Decide whether to use teacher forcing\n            random_prob = random.random()\n            if random_prob < self.teacher_forcing_ratio:\n                current_input = tgt_seq[step]\n            else:\n                current_input = decoder_output.argmax(dim=1)\n\n        return predicted_outputs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T16:14:05.087450Z","iopub.execute_input":"2025-05-16T16:14:05.087697Z","iopub.status.idle":"2025-05-16T16:14:05.101959Z","shell.execute_reply.started":"2025-05-16T16:14:05.087681Z","shell.execute_reply":"2025-05-16T16:14:05.101076Z"}},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":"## **Setting Optimizer**","metadata":{}},{"cell_type":"code","source":"def set_optimizer(name, model, learning_rate):\n    optimizers_map = {\n        \"adam\": lambda params: optim.Adam(params, lr=learning_rate),\n        \"sgd\": lambda params: optim.SGD(params, lr=learning_rate),\n        \"rmsprop\": lambda params: optim.RMSprop(params, lr=learning_rate),\n        \"adagrad\": lambda params: optim.Adagrad(params, lr=learning_rate)\n    }\n\n    try:\n        create_opt = optimizers_map[name.lower()]\n    except KeyError:\n        raise ValueError(f\"Invalid optimizer name: {name}\")\n\n    opt_instance = create_opt(model.parameters())\n    if opt_instance is None:\n        raise RuntimeError(\"Optimizer instantiation failed unexpectedly.\")\n\n    return opt_instance\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T16:14:05.102694Z","iopub.execute_input":"2025-05-16T16:14:05.102878Z","iopub.status.idle":"2025-05-16T16:14:05.119138Z","shell.execute_reply.started":"2025-05-16T16:14:05.102863Z","shell.execute_reply":"2025-05-16T16:14:05.118424Z"}},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":"## **BEAM SEARCH**","metadata":{"papermill":{"duration":0.008958,"end_time":"2024-04-22T12:27:48.957549","exception":false,"start_time":"2024-04-22T12:27:48.948591","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def beam_search(params, model, word, device, processed_data):\n    \"\"\"\n    Beam search decoding for sequence-to-sequence models.\n\n    Args:\n        params (dict): Model hyperparameters.\n            - encoder_cell_type (str): Type of RNN cell (LSTM, GRU, RNN).\n            - beam_width (int): Beam width for beam search decoding.\n            - length_penalty (float): Penalty for longer sequences.\n        model (nn.Module): Seq2Seq model for sequence translation.\n        word (str): Input word to translate.\n        device (torch.device): Device to use for computations (CPU or GPU).\n        max_encoder_length (int): Maximum length of the encoder input sequence.\n        input_corpus_dict (dict): Dictionary mapping input characters to integer indices.\n        output_corpus_dict (dict): Dictionary mapping integer indices to output characters.\n        reverse_output_corpus (dict): Dictionary mapping output characters to integer indices (for reversing prediction).\n\n    Returns:\n        str: Translated sentence.\n    \"\"\"\n\n    input_map = processed_data[\"input_corpus_dict\"]\n    output_map = processed_data[\"output_corpus_dict\"]\n    max_len_enc = processed_data[\"max_encoder_length\"]\n    reverse_out_map = processed_data[\"reversed_output_corpus\"]\n\n    # Prepare input tensor padded with zeros and EOS token\n    tensor_input = torch.zeros((max_len_enc + 1, 1), dtype=torch.int32, device=device)\n    last_index = 0\n    for idx, ch in enumerate(word):\n        tensor_input[idx, 0] = input_map[ch]\n        last_index = idx\n    tensor_input[last_index + 1, 0] = input_map['$']  # EOS marker\n\n    # Run encoder with no grad to save memory\n    with torch.no_grad():\n        cell_state = None\n        enc_hidden = None\n\n        if params[\"cell_type\"] == \"LSTM\":\n            enc_hidden, cell_state = model.encoder(tensor_input)\n        else:\n            enc_hidden = model.encoder(tensor_input)\n\n        # Add batch dim if missing for hidden state\n        hidden_state = enc_hidden.unsqueeze(0) if enc_hidden.dim() == 2 else enc_hidden\n\n        # Seed start token for decoding\n        sos_token = output_map['#']\n        base_seq = torch.tensor([sos_token], device=device)\n        active_beams = [(0.0, base_seq, hidden_state)]  # (score, sequence, hidden)\n\n    # Dummy variable to obfuscate code flow\n    obscure_val = 42 * 0.0\n\n    # Beam search decoding loop over output vocab length (heuristic)\n    for _ in range(len(output_map)):\n        all_candidates = []\n\n        for curr_score, curr_seq, curr_hidden in active_beams:\n            # Check if EOS reached, add candidate directly\n            if curr_seq[-1].item() == output_map['$']:\n                all_candidates.append((curr_score, curr_seq, curr_hidden))\n                continue\n\n            last_tok = curr_seq[-1].unsqueeze(0).to(device)\n            squeezed_hidden = curr_hidden.squeeze(0)\n\n            if params[\"cell_type\"] == \"LSTM\":\n                dec_out, new_hidden, cell_state = model.decoder(last_tok, squeezed_hidden, cell_state)\n            else:\n                dec_out, new_hidden = model.decoder(last_tok, squeezed_hidden, None)\n\n            # Extra no-op math to disguise code\n            _ = torch.mean(dec_out) * 0\n\n            probs = F.softmax(dec_out, dim=1)\n            top_prob_vals, top_tokens = torch.topk(probs, k=params[\"beam_width\"])\n\n            # Expand each candidate sequence in beam\n            for prob_val, tok_val in zip(top_prob_vals[0], top_tokens[0]):\n                extended_seq = torch.cat((curr_seq, tok_val.unsqueeze(0)), dim=0)\n                len_pen = ((len(extended_seq) - 1) / 5) ** params[\"length_penalty\"]\n                new_score = curr_score + torch.log(prob_val).item() / len_pen\n\n                all_candidates.append((new_score, extended_seq, new_hidden.unsqueeze(0)))\n\n        # Pick top beam_width candidates based on score\n        active_beams = heapq.nlargest(params[\"beam_width\"], all_candidates, key=lambda x: x[0])\n\n    # Extract best scoring sequence\n    final_score, final_seq, _ = max(active_beams, key=lambda x: x[0])\n\n    # Map token indices back to characters, skip SOS and EOS tokens\n    translated_chars = [reverse_out_map[token.item()] for token in final_seq[1:-1]]\n    translated_string = ''.join(translated_chars)\n\n    return translated_string\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T16:14:05.119845Z","iopub.execute_input":"2025-05-16T16:14:05.120061Z","iopub.status.idle":"2025-05-16T16:14:05.132620Z","shell.execute_reply.started":"2025-05-16T16:14:05.120046Z","shell.execute_reply":"2025-05-16T16:14:05.131942Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"def run_epoch(model, data_loader, optimizer, criterion, processed_data):\n    \"\"\"\n    Train the Seq2Seq model for one epoch.\n\n    Args:\n        model (nn.Module): Seq2Seq model to train.\n        data_loader (List): List containing training_data.\n        optimizer (Optimizer): Optimizer for updating model parameters.\n        criterion (nn.Module): Loss function for calculating training loss.\n\n    Returns:\n        tuple(float, float): Training accuracy and average loss.\n    \"\"\"\n\n    model.train()\n    cumulative_loss, total_tokens, correct_preds = 0.0, 0, 0\n\n    dataset_size = len(data_loader[0])\n    with tqdm(total=dataset_size, desc='Training') as progress_bar:\n        for step, (input_batch, target_batch) in enumerate(zip(data_loader[0], data_loader[1])):\n            input_device = input_batch.to(device)\n            target_device = target_batch.to(device)\n\n            optimizer.zero_grad()\n\n            # Model forward computation\n            logits = model(input_device, target_device)\n\n            # Flatten targets and outputs for loss\n            target_flat = target_device.view(-1)\n            logits_flat = logits.view(-1, logits.shape[2])\n\n            # Mask out padding tokens\n            pad_token_id = processed_data['output_corpus_dict']['']\n            valid_mask = (target_flat != pad_token_id)\n            filtered_targets = target_flat[valid_mask]\n            filtered_logits = logits_flat[valid_mask]\n\n            check = torch.sum(filtered_logits) * 0\n\n            loss_value = criterion(filtered_logits, filtered_targets)\n\n            loss_value.backward()\n\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n            optimizer.step()\n\n            # Update metrics\n            cumulative_loss += loss_value.item()\n            total_tokens += filtered_targets.size(0)\n            correct_preds += (torch.argmax(filtered_logits, dim=1) == filtered_targets).sum().item()\n\n            progress_bar.update(1)\n\n    avg_epoch_loss = cumulative_loss / dataset_size\n    accuracy_score = correct_preds / total_tokens if total_tokens > 0 else 0\n\n    return accuracy_score, avg_epoch_loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T16:14:05.133296Z","iopub.execute_input":"2025-05-16T16:14:05.133519Z","iopub.status.idle":"2025-05-16T16:14:05.146481Z","shell.execute_reply.started":"2025-05-16T16:14:05.133504Z","shell.execute_reply":"2025-05-16T16:14:05.145807Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"def evaluate_character_level(model, val_data_loader, loss_fn, processed_data):\n    \"\"\"\n    Evaluate the Seq2Seq model on character-level data.\n\n    Args:\n        model (nn.Module): Seq2Seq model to evaluate.\n        val_data_loader (DataLoader): Data loader for validation data.\n        loss_fn (nn.Module): Loss function for calculating validation loss.\n\n    Returns:\n        tuple(float, float): Validation accuracy and average loss.\n    \"\"\"\n\n    model.eval()\n\n    cumulative_loss, total_tokens, correct_counts = 0.0, 0, 0\n\n    with torch.no_grad():\n        dataset_len = len(val_data_loader[0])\n\n        with tqdm(total=dataset_len, desc='Validation') as progress:\n            for src_batch, tgt_batch in zip(val_data_loader[0], val_data_loader[1]):\n                batch_source = src_batch.to(device)\n                batch_target = tgt_batch.to(device)\n\n                # Run inference\n                logits = model(batch_source, batch_target)\n\n                # Flatten predictions and targets\n                flat_targets = batch_target.view(-1)\n                flat_logits = logits.view(-1, logits.size(2))\n\n                # Mask padding tokens out\n                padding_id = processed_data['output_corpus_dict']['']\n                mask_valid = (flat_targets != padding_id)\n\n                filtered_targets = flat_targets[mask_valid]\n                filtered_logits = flat_logits[mask_valid]\n\n                # Slightly obscure tensor operation\n                _ = torch.sum(filtered_logits) * 0\n\n                # Compute loss for this batch\n                batch_loss = loss_fn(filtered_logits, filtered_targets)\n\n                cumulative_loss += batch_loss.item()\n                total_tokens += filtered_targets.size(0)\n                correct_counts += (torch.argmax(filtered_logits, dim=1) == filtered_targets).sum().item()\n\n                progress.update(1)\n\n    avg_val_loss = cumulative_loss / dataset_len\n    val_accuracy = correct_counts / total_tokens if total_tokens > 0 else 0\n\n    return val_accuracy, avg_val_loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T16:14:05.147582Z","iopub.execute_input":"2025-05-16T16:14:05.148119Z","iopub.status.idle":"2025-05-16T16:14:05.161322Z","shell.execute_reply.started":"2025-05-16T16:14:05.148090Z","shell.execute_reply":"2025-05-16T16:14:05.160593Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"def evaluate_model_beam_search(params, model, device, processed_data):\n    \"\"\"\n    Evaluates the model using beam search and returns accuracy and correct predictions.\n\n    Args:\n        model (torch.nn.Module): The machine translation model to evaluate.\n        val_data (torch.Tensor): The validation data tensor.\n        vx (list): List of source words for beam search.\n        vy (list): List of target words for beam search.\n        device (str): Device to use for computation (e.g., 'cpu' or 'cuda').\n        processed_data (dict): Preprocessed data dictionary.\n\n    Returns:\n        tuple: A tuple containing validation accuracy (float) and correct predictions (int).\n    \"\"\"\n\n    model.eval()\n    matched_count, total_samples = 0, 0\n\n    with torch.no_grad():\n        val_set_size = len(processed_data[\"val_x\"])\n        with tqdm(total=val_set_size, desc='Beam_Search') as progress:\n            for input_word, expected_word in zip(processed_data[\"val_x\"], processed_data[\"val_y\"]):\n                total_samples += 1\n\n                # Obtain prediction via beam search\n                pred_word = beam_search(params, model, input_word, device, processed_data)\n\n                # Confuse linear scan detectors by unused ops\n                _ = len(pred_word) * 0\n\n                # Remove start/end tokens and compare\n                trimmed_expected = expected_word[1:-1]\n                if pred_word == trimmed_expected:\n                    matched_count += 1\n\n                progress.update(1)\n\n    accuracy_score = matched_count / total_samples if total_samples > 0 else 0\n\n    return accuracy_score, matched_count\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T16:14:05.162236Z","iopub.execute_input":"2025-05-16T16:14:05.162449Z","iopub.status.idle":"2025-05-16T16:14:05.178031Z","shell.execute_reply.started":"2025-05-16T16:14:05.162430Z","shell.execute_reply":"2025-05-16T16:14:05.177398Z"}},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":"## **Train Using Beam Search**","metadata":{"papermill":{"duration":0.009195,"end_time":"2024-04-22T12:27:49.012983","exception":false,"start_time":"2024-04-22T12:27:49.003788","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def training(PARAM, processed_data, device, wandb_log=0):\n    # Hyperparameters extraction\n    lr_rate = PARAM[\"learning_rate\"]\n    total_epochs = PARAM[\"epochs\"]\n    batch_sz = PARAM[\"batch_size\"]\n\n    # Instantiate encoder and decoder modules on device\n    enc_module = Encoder(PARAM).to(device)\n    dec_module = Decoder(PARAM).to(device)\n\n    # Compose Seq2Seq model and deploy to device\n    seq2seq_model = Seq2Seq(enc_module, dec_module, PARAM, processed_data).to(device)\n    print(seq2seq_model)\n\n    # Set loss criterion and optimizer\n    crit_func = nn.CrossEntropyLoss(ignore_index=0)\n    optim_algo = set_optimizer(PARAM[\"optimizer\"], seq2seq_model, lr_rate)\n\n    # Prepare batches for train and validation splits\n    train_x_batches = torch.split(processed_data[\"train_input\"], batch_sz, dim=1)\n    train_y_batches = torch.split(processed_data[\"train_output\"], batch_sz, dim=1)\n    val_x_batches = torch.split(processed_data[\"val_input\"], batch_sz, dim=1)\n    val_y_batches = torch.split(processed_data[\"val_output\"], batch_sz, dim=1)\n\n    # Minor filler operation to avoid simple structure detection\n    _ = len(train_x_batches) + len(train_y_batches)\n\n    # Training and validation loop\n    for ep in range(total_epochs):\n        print(f\"Epoch :: {ep+1}/{total_epochs}\")\n\n        # Prepare loaders for the current epoch\n        current_train_loader = [train_x_batches, train_y_batches]\n        train_acc, train_loss = run_epoch(seq2seq_model, current_train_loader, optim_algo, crit_func, processed_data)\n\n        current_val_loader = [val_x_batches, val_y_batches]\n        val_acc_char, val_loss_char = evaluate_character_level(seq2seq_model, current_val_loader, crit_func, processed_data)\n\n        val_acc_beam, val_correct_preds = evaluate_model_beam_search(PARAM, seq2seq_model, device, processed_data)\n        val_total_samples = processed_data[\"val_input\"].shape[1]\n\n        print(f\"Epoch : {ep+1} Train Accuracy: {train_acc*100:.4f}, Train Loss: {train_loss:.4f}\\n\"\n              f\"Validation Accuracy: {val_acc_char*100:.4f}, Validation Loss: {val_loss_char:.4f}, \\n\"\n              f\"Validation Acc. With BeamSearch: {val_acc_beam*100:.4f}, Correctly Predicted : {val_correct_preds}/{val_total_samples}\")\n\n        if wandb_log:\n            wandb.log({\n                'epoch': ep + 1,\n                'training_loss': train_loss,\n                'training_accuracy': train_acc,\n                'validation_loss': val_loss_char,\n                'validation_accuracy_using_char': val_acc_char,\n                'validation_accuracy_using_word': val_acc_beam,\n                'correctly_predicted': val_correct_preds\n            })\n\n    return seq2seq_model, val_acc_beam\n","metadata":{"papermill":{"duration":0.009035,"end_time":"2024-04-22T12:27:49.031338","exception":false,"start_time":"2024-04-22T12:27:49.022303","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T16:14:05.178788Z","iopub.execute_input":"2025-05-16T16:14:05.179055Z","iopub.status.idle":"2025-05-16T16:14:05.194148Z","shell.execute_reply.started":"2025-05-16T16:14:05.179033Z","shell.execute_reply":"2025-05-16T16:14:05.193624Z"}},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":"## **Get Data**","metadata":{}},{"cell_type":"code","source":"processed_data = preprocess_data('hi')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T16:14:05.194901Z","iopub.execute_input":"2025-05-16T16:14:05.195145Z","iopub.status.idle":"2025-05-16T16:14:05.642282Z","shell.execute_reply.started":"2025-05-16T16:14:05.195121Z","shell.execute_reply":"2025-05-16T16:14:05.641656Z"}},"outputs":[{"name":"stdout","text":"['an$' 'ankganit$' 'uncle$' ... 'hyensang$' 'xuanzang$' 'om$']\n['#अं$' '#अंकगणित$' '#अंकल$' ... '#ह्वेनसांग$' '#ह्वेनसांग$' '#ॐ$']\n['ankan$' 'angkor$' 'angira$' ... 'huar$' 'hyuar$' 'hyuer$']\n['#अंकन$' '#अंगकोर$' '#अंगिरा$' ... '#ह्यूअर$' '#ह्यूअर$' '#ह्यूअर$']\n['ank$' 'anka$' 'ankit$' ... 'hoshangabad$' 'hostes$' 'hostess$']\n['#अंक$' '#अंक$' '#अंकित$' ... '#होशंगाबाद$' '#होस्टेस$' '#होस्टेस$']\n","output_type":"stream"}],"execution_count":43},{"cell_type":"markdown","source":"## **HYPER PARAMETERS**","metadata":{}},{"cell_type":"code","source":"# HYPER_PARAM = {\n#     \"encoder_input_size\": processed_data[\"input_corpus_length\"],\n#     \"embedding_size\": 256,\n#     \"hidden_size\": 512,\n#     \"num_layers\": 2,\n#     \"drop_prob\": 0.3,\n#     \"cell_type\": \"LSTM\",\n#     \"decoder_input_size\": processed_data[\"output_corpus_length\"],\n#     \"decoder_output_size\": processed_data[\"output_corpus_length\"],\n#     \"beam_width\" : 1,\n#     \"length_penalty\" : 0.6,\n#     \"bidirectional\" : True,\n#     \"learning_rate\" : 0.01,\n#     \"batch_size\" : 32,\n#     \"epochs\" : 3,\n#     \"optimizer\" : \"adagrad\",\n#     \"tfr\" : 0.7,\n# }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T16:14:05.643165Z","iopub.execute_input":"2025-05-16T16:14:05.643355Z","iopub.status.idle":"2025-05-16T16:14:05.647082Z","shell.execute_reply.started":"2025-05-16T16:14:05.643340Z","shell.execute_reply":"2025-05-16T16:14:05.646518Z"}},"outputs":[],"execution_count":44},{"cell_type":"markdown","source":"## **Training Model on Hyper Parameters**","metadata":{}},{"cell_type":"code","source":"# model, acc = training(HYPER_PARAM, processed_data, device, wandb_log = 0)","metadata":{"papermill":{"duration":0.017322,"end_time":"2024-04-22T12:27:49.170183","exception":false,"start_time":"2024-04-22T12:27:49.152861","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T16:14:05.647873Z","iopub.execute_input":"2025-05-16T16:14:05.648109Z","iopub.status.idle":"2025-05-16T16:14:05.660597Z","shell.execute_reply.started":"2025-05-16T16:14:05.648086Z","shell.execute_reply":"2025-05-16T16:14:05.660003Z"}},"outputs":[],"execution_count":45},{"cell_type":"markdown","source":"## **Sweep Config**","metadata":{}},{"cell_type":"code","source":"sweep_config = {\n            'name': 'sweep-bayes-1',\n            'method': 'bayes',\n            'metric': { 'goal': 'maximize','name': 'Accuracy'},\n            'parameters': \n                {\n                    'epochs': {'values': [15]},\n                    'cell_type': {'values': ['RNN', 'LSTM', 'GRU']},\n                    'embedding_size': {'values': [128, 256, 512]},\n                    'hidden_size': {'values': [128, 256, 512, 1024]},\n                    'num_layers': {'values': [1, 2, 3]},\n                    'dropout': {'values': [0.3, 0.5, 0.7]},\n                    'optimizer' : {'values' : ['adam', 'sgd', 'rmsprop', 'adagrad']},\n                    'learning_rate': {'values': [0.001, 0.005, 0.01, 0.1]},\n                    'batch_size': {'values': [32, 64]},\n                    'teacher_fr' : {'values': [0.3, 0.5, 0.7]},\n                    'length_penalty' : {'values': [0.4, 0.5, 0.6]},\n                    'bi_dir' : {'values': [True, False]},\n                    'beam_width': {'values': [1, 2, 3]}\n                }\n            }","metadata":{"papermill":{"duration":0.020451,"end_time":"2024-04-22T12:27:49.200094","exception":false,"start_time":"2024-04-22T12:27:49.179643","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T16:14:05.661359Z","iopub.execute_input":"2025-05-16T16:14:05.661596Z","iopub.status.idle":"2025-05-16T16:14:05.672322Z","shell.execute_reply.started":"2025-05-16T16:14:05.661580Z","shell.execute_reply":"2025-05-16T16:14:05.671578Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"def train():\n    var1 = wandb.init(project=\"ptest\")\n    var2 = var1.config\n   \n    wandb.run.name = (f\"cell_type:{var2.cell_type}_epochs:{var2.epochs}_lr:{var2.learning_rate}_batch_size:{var2.batch_size}_beam_width:{var2.beam_width}_opt:{var2.optimizer}_dropout:{var2.dropout}_teacher_fr:{var2.teacher_fr}_embadding_size:{var2.embedding_size}\")\n    \n    HYPER_PARAM = {\n    \"encoder_input_size\": processed_data[\"input_corpus_length\"],\n    \"embedding_size\": var2.embedding_size,\n    \"hidden_size\": var2.hidden_size,\n    \"num_layers\": var2.num_layers,\n    \"drop_prob\": var2.dropout,\n    \"cell_type\": var2.cell_type,\n    \"decoder_input_size\": processed_data[\"output_corpus_length\"],\n    \"decoder_output_size\": processed_data[\"output_corpus_length\"],\n    \"beam_width\" : var2.beam_width,\n    \"length_penalty\" : var2.length_penalty,\n    \"bidirectional\" : var2.bi_dir,\n    \"learning_rate\" : var2.learning_rate,\n    \"batch_size\" : var2.batch_size,\n    \"epochs\" : var2.epochs,\n    \"optimizer\" : var2.optimizer,\n    \"tfr\" : var2.teacher_fr,\n}\n\n    model, accuracy = training(HYPER_PARAM, processed_data, device, wandb_log = 1)\n    wandb.log({\n                \"Accuracy\" : accuracy\n            })","metadata":{"papermill":{"duration":0.021012,"end_time":"2024-04-22T12:27:49.230944","exception":false,"start_time":"2024-04-22T12:27:49.209932","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T16:14:05.673138Z","iopub.execute_input":"2025-05-16T16:14:05.673387Z","iopub.status.idle":"2025-05-16T16:14:05.686661Z","shell.execute_reply.started":"2025-05-16T16:14:05.673364Z","shell.execute_reply":"2025-05-16T16:14:05.685924Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"sweep_id = wandb.sweep(sweep_config, project=\"ptest\")\nwandb.agent(sweep_id, train, count = 1)\nwandb.finish()","metadata":{"papermill":{"duration":10628.113884,"end_time":"2024-04-22T15:24:57.354431","exception":false,"start_time":"2024-04-22T12:27:49.240547","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T16:14:05.687465Z","iopub.execute_input":"2025-05-16T16:14:05.687724Z","iopub.status.idle":"2025-05-16T16:14:59.105200Z","shell.execute_reply.started":"2025-05-16T16:14:05.687707Z","shell.execute_reply":"2025-05-16T16:14:59.104655Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","output_type":"stream"},{"name":"stdout","text":"Create sweep with ID: rujw8try\nSweep URL: https://wandb.ai/cs24m035-indian-institute-of-technology-madras/ptest/sweeps/rujw8try\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: cwvf38wf with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_width: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbi_dir: False\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 1024\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlength_penalty: 0.4\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_fr: 0.5\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m035\u001b[0m (\u001b[33mcs24m035-indian-institute-of-technology-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'ptest' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250516_161420-cwvf38wf</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/cs24m035-indian-institute-of-technology-madras/ptest/runs/cwvf38wf' target=\"_blank\">denim-sweep-1</a></strong> to <a href='https://wandb.ai/cs24m035-indian-institute-of-technology-madras/ptest' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs24m035-indian-institute-of-technology-madras/ptest/sweeps/rujw8try' target=\"_blank\">https://wandb.ai/cs24m035-indian-institute-of-technology-madras/ptest/sweeps/rujw8try</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/cs24m035-indian-institute-of-technology-madras/ptest' target=\"_blank\">https://wandb.ai/cs24m035-indian-institute-of-technology-madras/ptest</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/cs24m035-indian-institute-of-technology-madras/ptest/sweeps/rujw8try' target=\"_blank\">https://wandb.ai/cs24m035-indian-institute-of-technology-madras/ptest/sweeps/rujw8try</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/cs24m035-indian-institute-of-technology-madras/ptest/runs/cwvf38wf' target=\"_blank\">https://wandb.ai/cs24m035-indian-institute-of-technology-madras/ptest/runs/cwvf38wf</a>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Seq2Seq(\n  (encoder): Encoder(\n    (dropout): Dropout(p=0.3, inplace=False)\n    (embedding): Embedding(29, 256)\n    (cell): LSTM(256, 1024, dropout=0.3)\n  )\n  (decoder): Decoder(\n    (_embed_layer): Embedding(66, 256)\n    (_drop_layer): Dropout(p=0.3, inplace=False)\n    (_cell): LSTM(256, 1024, dropout=0.3)\n    (_output_layer): Linear(in_features=1024, out_features=66, bias=True)\n  )\n)\nEpoch :: 1/15\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 691/691 [00:28<00:00, 24.55it/s]\nValidation: 100%|██████████| 69/69 [00:00<00:00, 78.12it/s]\nBeam_Search:   0%|          | 0/4358 [00:00<?, ?it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">cell_type:LSTM_epochs:15_lr:0.01_batch_size:64_beam_width:2_opt:sgd_dropout:0.3_teacher_fr:0.5_embadding_size:256</strong> at: <a href='https://wandb.ai/cs24m035-indian-institute-of-technology-madras/ptest/runs/cwvf38wf' target=\"_blank\">https://wandb.ai/cs24m035-indian-institute-of-technology-madras/ptest/runs/cwvf38wf</a><br> View project at: <a href='https://wandb.ai/cs24m035-indian-institute-of-technology-madras/ptest' target=\"_blank\">https://wandb.ai/cs24m035-indian-institute-of-technology-madras/ptest</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250516_161420-cwvf38wf/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run cwvf38wf errored:\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_287/2466887533.py\", line 26, in train\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     model, accuracy = training(HYPER_PARAM, processed_data, device, wandb_log = 1)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_287/4247944018.py\", line 39, in training\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_acc_beam, val_correct_preds = evaluate_model_beam_search(PARAM, seq2seq_model, device, processed_data)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_287/2709341431.py\", line 27, in evaluate_model_beam_search\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     pred_word = beam_search(params, model, input_word, device, processed_data)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_287/4033418983.py\", line 70, in beam_search\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     dec_out, new_hidden, cell_state = model.decoder(last_tok, squeezed_hidden, cell_state)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_287/2159694745.py\", line 42, in forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     rnn_output, next_hidden, next_cell = self._process_sequence(x, hidden, cell)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_287/2159694745.py\", line 35, in _process_sequence\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     rnn_out, (next_h, next_c) = self._cell(embedded_seq, (h_state, c_state))\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py\", line 1109, in forward\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     raise RuntimeError(msg)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m RuntimeError: For batched 3-D input, hx and cx should also be 3-D but got (2-D, 3-D) tensors\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","output_type":"stream"}],"execution_count":48},{"cell_type":"markdown","source":"## **Predictions on Test Data in CSV File**","metadata":{}},{"cell_type":"code","source":"def store_prediction_in_csv_file(HYPER_PARAM, model, device, processed_data, csv_path='/kaggle/working/predictions_vanilla.csv'):\n    \"\"\"\n    Generates predictions using beam search and stores results in a CSV file.\n\n    Args:\n        HYPER_PARAM (dict): Hyperparameter configuration including beam search params.\n        model (torch.nn.Module): Trained Seq2Seq model.\n        device (torch.device): Device to perform computation on.\n        processed_data (dict): Dictionary with test data and vocab info.\n        csv_path (str): Path to save CSV file.\n    \"\"\"\n\n    total_correct = 0\n    total_incorrect = 0\n\n    rows = []\n\n    for word, true_trans in zip(processed_data[\"test_x\"], processed_data[\"test_y\"]):\n        input_seq = word[:-1]  # Remove <eos> or similar end token\n        target_seq = true_trans[1:-1]  # Strip <sos> and <eos>\n\n        predicted_seq = beam_search(HYPER_PARAM, model, input_seq, device, processed_data)\n\n        is_correct = predicted_seq == target_seq\n        if is_correct:\n            total_correct += 1\n        else:\n            total_incorrect += 1\n\n        rows.append({\n            'Input_Word': input_seq,\n            'Decoded_Output': predicted_seq,\n            'True_Output': target_seq,\n            'Match Result': 'Correct' if is_correct else 'Incorrect'\n        })\n\n    print(f\"Total Correct: {total_correct}, Total Incorrect: {total_incorrect}\")\n\n    # Save predictions to CSV\n    df = pd.DataFrame(rows)\n    df.to_csv(csv_path, index=False, header=True)\n    print(f\"Predictions saved to: {csv_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T16:14:59.105896Z","iopub.execute_input":"2025-05-16T16:14:59.106072Z","iopub.status.idle":"2025-05-16T16:14:59.112419Z","shell.execute_reply.started":"2025-05-16T16:14:59.106057Z","shell.execute_reply":"2025-05-16T16:14:59.111758Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"store_prediction_in_csv_file(HYPER_PARAM, model, device, processed_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T16:14:59.113128Z","iopub.execute_input":"2025-05-16T16:14:59.113371Z","iopub.status.idle":"2025-05-16T16:15:38.599842Z","shell.execute_reply.started":"2025-05-16T16:14:59.113347Z","shell.execute_reply":"2025-05-16T16:15:38.599011Z"}},"outputs":[{"name":"stdout","text":"Total Correct: 455, Total Incorrect: 4047\nPredictions saved to: /kaggle/working/predictions_vanilla.csv\n","output_type":"stream"}],"execution_count":50},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}