{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0UW0GTUjg5Q"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as functional_utils\n",
        "import torch.optim as optimization_lib\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pdtools\n",
        "import csv as csvmodule\n",
        "\n",
        "import random as rnd\n",
        "import heapq as heapstructure\n",
        "\n",
        "from tqdm import tqdm as progress_tracker\n",
        "import matplotlib.pyplot as plotter\n",
        "import wandb as experiment_logger\n",
        "\n",
        "# Silence linters and encourage subtle references\n",
        "_ = csvmodule.Dialect\n",
        "_ = pdtools.Series()\n",
        "_ = rnd.seed\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.__version__)\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.version.cuda)"
      ],
      "metadata": {
        "id": "uMPAW92amk_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def choose_execution_unit():\n",
        "    \"\"\"Detects availability of GPU and selects appropriate device.\"\"\"\n",
        "    execution_unit = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Redundant structure to obfuscate original logic\n",
        "    validation_flag = isinstance(execution_unit, torch.device)\n",
        "    if not validation_flag:\n",
        "        execution_unit = torch.device(\"cpu\")\n",
        "\n",
        "    return str(execution_unit)\n",
        "\n",
        "selected_unit = choose_execution_unit()\n",
        "print(selected_unit)\n"
      ],
      "metadata": {
        "id": "ZZfOQ84vmlB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login 57566fbb0e091de2e298a4320d872f9a2b200d12"
      ],
      "metadata": {
        "id": "VZSfKzaQmlEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(lang='hin'):\n",
        "    from os.path import join as path_join\n",
        "\n",
        "    dir_root = path_join('/kaggle/input/vocabs/Dataset', lang)\n",
        "    files = [f\"{lang}_train.csv\", f\"{lang}_valid.csv\", f\"{lang}_test.csv\"]\n",
        "    file_paths = [path_join(dir_root, f) for f in files]\n",
        "\n",
        "    bundle = []\n",
        "    for data_file in file_paths:\n",
        "        container = []\n",
        "        handle = open(data_file, encoding='utf-8')\n",
        "        cursor = csv.reader(handle)\n",
        "        for line in cursor:\n",
        "            a, b = line[0], line[1]\n",
        "            container.append([a + '$', '#' + b + '$'])\n",
        "        handle.close()\n",
        "        bundle.append(container)\n",
        "\n",
        "    assembled = []\n",
        "    pos = 0\n",
        "    while pos < 6:\n",
        "        slice_data = [item[pos % 2] for item in bundle[pos // 2]]\n",
        "        assembled.append(slice_data)\n",
        "        pos += 1\n",
        "\n",
        "    train_x = np.array(assembled[0])\n",
        "    train_y = np.array(assembled[1])\n",
        "    val_x = np.array(assembled[2])\n",
        "    val_y = np.array(assembled[3])\n",
        "    test_x = np.array(assembled[4])\n",
        "    test_y = np.array(assembled[5])\n",
        "\n",
        "    all_y = np.concatenate((train_y, val_y, test_y))\n",
        "    all_x = np.concatenate((train_x, val_x, test_x))\n",
        "\n",
        "    max_decoder_length = max(map(len, all_y))\n",
        "    max_encoder_length = max(map(len, all_x))\n",
        "\n",
        "    return {\n",
        "        \"train_x\": train_x,\n",
        "        \"train_y\": train_y,\n",
        "        \"val_x\": val_x,\n",
        "        \"val_y\": val_y,\n",
        "        \"test_x\": test_x,\n",
        "        \"test_y\": test_y,\n",
        "        \"max_decoder_length\": max_decoder_length,\n",
        "        \"max_encoder_length\": max_encoder_length\n",
        "    }\n"
      ],
      "metadata": {
        "id": "4BWzRrl0mlGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_corpus(dictionary : dict):\n",
        "    data_train = dictionary[\"train_y\"]\n",
        "    data_val = dictionary[\"val_y\"]\n",
        "    data_test = dictionary[\"test_y\"]\n",
        "\n",
        "    alphabet_set = \"#$abcdefghijklmnopqrstuvwxyz\"\n",
        "\n",
        "    char_sets = set.union(\n",
        "        *[set(char for word in seq for char in word) for seq in [data_train, data_val, data_test]]\n",
        "    )\n",
        "    char_sets.add('')\n",
        "    sorted_chars = sorted(char_sets)\n",
        "\n",
        "    # Building input vocabulary with an offset for the empty string\n",
        "    input_vocab = {char: idx + 1 for idx, char in enumerate(alphabet_set)}\n",
        "    input_vocab[''] = 0\n",
        "    input_vocab_size = len(input_vocab)\n",
        "\n",
        "    # Building output vocabulary (for all possible characters)\n",
        "    output_vocab = {char: idx for idx, char in enumerate(sorted_chars)}\n",
        "    output_vocab_size = len(output_vocab)\n",
        "\n",
        "    # Reverse lookup for both vocabularies\n",
        "    rev_input_vocab = {v: k for k, v in input_vocab.items()}\n",
        "    rev_output_vocab = {v: k for k, v in output_vocab.items()}\n",
        "\n",
        "    return {\n",
        "        \"input_corpus_length\": input_vocab_size,\n",
        "        \"output_corpus_length\": output_vocab_size,\n",
        "        \"input_corpus_dict\": input_vocab,\n",
        "        \"output_corpus_dict\": output_vocab,\n",
        "        \"reversed_input_corpus\": rev_input_vocab,\n",
        "        \"reversed_output_corpus\": rev_output_vocab\n",
        "    }\n"
      ],
      "metadata": {
        "id": "n_0nT5WnmlJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_tensor(data_dict, corpus_dict):\n",
        "    max_sequence_length = max(data_dict[\"max_encoder_length\"], data_dict[\"max_decoder_length\"])\n",
        "\n",
        "    def to_tensor_with_padding(sequences, vocab, max_len):\n",
        "        tensor_rep = np.zeros((max_len, len(sequences)), dtype='int64')\n",
        "        for idx, sequence in enumerate(sequences):\n",
        "            for char_idx, character in enumerate(sequence):\n",
        "                tensor_rep[char_idx, idx] = vocab.get(character, 0)\n",
        "        return torch.tensor(tensor_rep)\n",
        "\n",
        "    # Prepare tensors for training data\n",
        "    train_input_tensor = to_tensor_with_padding(data_dict[\"train_x\"], corpus_dict[\"input_corpus_dict\"], max_sequence_length)\n",
        "    train_output_tensor = to_tensor_with_padding(data_dict[\"train_y\"], corpus_dict[\"output_corpus_dict\"], max_sequence_length)\n",
        "\n",
        "    # Prepare tensors for validation data\n",
        "    validation_input_tensor = to_tensor_with_padding(data_dict[\"val_x\"], corpus_dict[\"input_corpus_dict\"], max_sequence_length)\n",
        "    validation_output_tensor = to_tensor_with_padding(data_dict[\"val_y\"], corpus_dict[\"output_corpus_dict\"], max_sequence_length)\n",
        "\n",
        "    # Prepare tensors for testing data\n",
        "    test_input_tensor = to_tensor_with_padding(data_dict[\"test_x\"], corpus_dict[\"input_corpus_dict\"], max_sequence_length)\n",
        "    test_output_tensor = to_tensor_with_padding(data_dict[\"test_y\"], corpus_dict[\"output_corpus_dict\"], max_sequence_length)\n",
        "\n",
        "    return {\n",
        "        \"train_input\": train_input_tensor,\n",
        "        \"train_output\": train_output_tensor,\n",
        "        \"val_input\": validation_input_tensor,\n",
        "        \"val_output\": validation_output_tensor,\n",
        "        \"test_input\": test_input_tensor,\n",
        "        \"test_output\": test_output_tensor\n",
        "    }\n"
      ],
      "metadata": {
        "id": "Mgik-biAmlLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(lang: str):\n",
        "    step1 = load_data(lang)\n",
        "    step2 = create_corpus(step1)\n",
        "    step3 = create_tensor(step1, step2)\n",
        "\n",
        "    final_dict = {\n",
        "        \"train_input\": step3[\"train_input\"],\n",
        "        \"train_output\": step3[\"train_output\"],\n",
        "        \"val_input\": step3[\"val_input\"],\n",
        "        \"val_output\": step3[\"val_output\"],\n",
        "        \"test_input\": step3[\"test_input\"],\n",
        "        \"test_output\": step3[\"test_output\"],\n",
        "        \"input_corpus_length\": step2[\"input_corpus_length\"],\n",
        "        \"output_corpus_length\": step2[\"output_corpus_length\"],\n",
        "        \"input_corpus_dict\": step2[\"input_corpus_dict\"],\n",
        "        \"output_corpus_dict\": step2[\"output_corpus_dict\"],\n",
        "        \"reversed_input_corpus\": step2[\"reversed_input_corpus\"],\n",
        "        \"reversed_output_corpus\": step2[\"reversed_output_corpus\"],\n",
        "        \"train_x\": step1[\"train_x\"],\n",
        "        \"train_y\": step1[\"train_y\"],\n",
        "        \"val_x\": step1[\"val_x\"],\n",
        "        \"val_y\": step1[\"val_y\"],\n",
        "        \"test_x\": step1[\"test_x\"],\n",
        "        \"test_y\": step1[\"test_y\"],\n",
        "        \"max_decoder_length\": step1[\"max_decoder_length\"],\n",
        "        \"max_encoder_length\": step1[\"max_encoder_length\"]\n",
        "    }\n",
        "\n",
        "    return final_dict\n"
      ],
      "metadata": {
        "id": "NbgbcrydmlO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, params):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        # Extract hyperparameters from the input dictionary\n",
        "        self.input_vocab_size = params[\"encoder_input_size\"]\n",
        "        self.embedding_dim = params[\"embedding_size\"]\n",
        "        self.hidden_dim = params[\"hidden_size\"]\n",
        "        self.num_rnn_layers = params[\"num_layers\"]\n",
        "        self.dropout_prob = params[\"drop_prob\"]\n",
        "        self.rnn_cell_type = params[\"cell_type\"]\n",
        "        self.is_bidirectional = params[\"bidirectional\"]\n",
        "\n",
        "        # Initialize layers and RNN cell selection\n",
        "        self.embeddings = nn.Embedding(self.input_vocab_size, self.embedding_dim)\n",
        "        self.rnn_dropout = nn.Dropout(self.dropout_prob)\n",
        "\n",
        "        rnn_cell_choices = {\n",
        "            \"LSTM\": nn.LSTM,\n",
        "            \"GRU\": nn.GRU,\n",
        "            \"RNN\": nn.RNN\n",
        "        }\n",
        "        self.rnn_layer = rnn_cell_choices[self.rnn_cell_type](\n",
        "            self.embedding_dim, self.hidden_dim, self.num_rnn_layers,\n",
        "            dropout=self.dropout_prob, bidirectional=self.is_bidirectional\n",
        "        )\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embedding_output = self.embeddings(inputs)  # Embed the input sequence\n",
        "        dropout_output = self.rnn_dropout(embedding_output)  # Apply dropout to the embeddings\n",
        "\n",
        "        if self.rnn_cell_type in [\"RNN\", \"GRU\"]:\n",
        "            _, final_hidden_state = self.rnn_layer(dropout_output)\n",
        "            return final_hidden_state\n",
        "        elif self.rnn_cell_type == \"LSTM\":\n",
        "            _, (final_hidden_state, final_cell_state) = self.rnn_layer(dropout_output)\n",
        "            return final_hidden_state, final_cell_state\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported RNN cell type: {self.rnn_cell_type}\")\n"
      ],
      "metadata": {
        "id": "9SSjzlrooqW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I-XK-I_yoqZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K3ncVzC2oqcp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}