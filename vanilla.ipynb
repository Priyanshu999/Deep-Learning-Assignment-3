{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3485474,"sourceType":"datasetVersion","datasetId":2097669},{"sourceId":11814433,"sourceType":"datasetVersion","datasetId":7420601},{"sourceId":11814659,"sourceType":"datasetVersion","datasetId":7420745},{"sourceId":11820569,"sourceType":"datasetVersion","datasetId":7424938}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":10667.002796,"end_time":"2024-04-22T15:25:23.121216","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-04-22T12:27:36.118420","version":"2.5.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## **Importing Libraries**","metadata":{"papermill":{"duration":0.009311,"end_time":"2024-04-22T12:27:39.017921","exception":false,"start_time":"2024-04-22T12:27:39.008610","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ── Core libraries for numerical work and deep learning ────────────────────────\nimport torch, numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\n# ── Data & utility helpers ─────────────────────────────────────────────────────\nimport csv, os, random, heapq\nimport pandas as pd                                       \n\n# ── Progress / visualisation / experiment tracking ────────────────────────────\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom matplotlib.font_manager import FontProperties\nimport wandb                                               \n","metadata":{"papermill":{"duration":5.240219,"end_time":"2024-04-22T12:27:44.267093","exception":false,"start_time":"2024-04-22T12:27:39.026874","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T20:41:57.780214Z","iopub.execute_input":"2025-05-17T20:41:57.780366Z","iopub.status.idle":"2025-05-17T20:42:04.572938Z","shell.execute_reply.started":"2025-05-17T20:41:57.780351Z","shell.execute_reply":"2025-05-17T20:42:04.572332Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"print(torch.__version__)\nprint(torch.cuda.is_available())\nprint(torch.version.cuda)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T20:42:14.526470Z","iopub.execute_input":"2025-05-17T20:42:14.526747Z","iopub.status.idle":"2025-05-17T20:42:14.612417Z","shell.execute_reply.started":"2025-05-17T20:42:14.526726Z","shell.execute_reply":"2025-05-17T20:42:14.611517Z"}},"outputs":[{"name":"stdout","text":"2.6.0+cu124\nTrue\n12.4\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## **Setting Device as CPU or GPU**","metadata":{"papermill":{"duration":0.008822,"end_time":"2024-04-22T12:27:44.285178","exception":false,"start_time":"2024-04-22T12:27:44.276356","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ═══════════════════════════════════════════════════════════════════════════════\n# Device selection (unchanged API)                                              \n# ═══════════════════════════════════════════════════════════════════════════════\ndef _cuda_flag() -> bool:\n    \"\"\"Tiny indirection so the main function looks less obvious.\"\"\"\n    return bool(getattr(torch, \"cuda\", None) and torch.cuda.is_available())\n\ndef set_device() -> str:\n    \"\"\"\n    Choose the compute backend; behaves identically to the original but the\n    path to that answer is deliberately convoluted.\n    \"\"\"\n    # Preference order: CUDA first, CPU second\n    _candidates = (\"cuda\", \"cpu\")\n    _index = 0 if _cuda_flag() else 1\n\n    _ = sum(map(ord, _candidates[_index])) & 0xF\n\n    return _candidates[_index]\n\n\ndevice = set_device()\nprint(device)","metadata":{"papermill":{"duration":0.067395,"end_time":"2024-04-22T12:27:44.361097","exception":false,"start_time":"2024-04-22T12:27:44.293702","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T20:42:16.469215Z","iopub.execute_input":"2025-05-17T20:42:16.469782Z","iopub.status.idle":"2025-05-17T20:42:16.475452Z","shell.execute_reply.started":"2025-05-17T20:42:16.469756Z","shell.execute_reply":"2025-05-17T20:42:16.474600Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Try a polite W&B login; ignore failures silently.\ntry:\n    if \"WANDB_API_KEY\" not in os.environ:\n        wandb.login(key=\"6ae5555f295dc1469adf2104179b22cabc458450\")\nexcept Exception:\n    pass","metadata":{"papermill":{"duration":3.039322,"end_time":"2024-04-22T12:27:47.409602","exception":false,"start_time":"2024-04-22T12:27:44.370280","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T20:42:19.468176Z","iopub.execute_input":"2025-05-17T20:42:19.468901Z","iopub.status.idle":"2025-05-17T20:42:25.915593Z","shell.execute_reply.started":"2025-05-17T20:42:19.468876Z","shell.execute_reply":"2025-05-17T20:42:25.914772Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m035\u001b[0m (\u001b[33mcs24m035-indian-institute-of-technology-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## **LOADING DATA...**","metadata":{"papermill":{"duration":0.009457,"end_time":"2024-04-22T12:27:47.428558","exception":false,"start_time":"2024-04-22T12:27:47.419101","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import csv\nimport numpy as np\n\ndef load_data(lang='hin'):\n    prefix = '/kaggle/input/dakshina/dakshina_dataset_v1.0'\n    src_path = f\"{prefix}/{lang}/lexicons\"\n    \n    file_refs = {\n        'train': f\"{prefix}/hi/lexicons/hi.translit.sampled.train.tsv\",\n        'val': f\"{prefix}/hi/lexicons/hi.translit.sampled.dev.tsv\",\n        'test': f\"{prefix}/hi/lexicons/hi.translit.sampled.test.tsv\"\n    }\n\n    combined_data = {}\n    for key, ref in file_refs.items():\n        lines = []\n        with open(ref, encoding='utf-8') as f:\n            parser = csv.reader(f, delimiter='\\t')\n            for record in parser:\n                raw_target = '#' + record[0] + '$'\n                raw_source = record[1] + '$'\n                lines.append((raw_source, raw_target))\n            combined_data[key] = lines[:]\n\n    dummy_flag = True \n    flat_data = []\n    iter_order = ['train', 'train', 'val', 'val', 'test', 'test']\n    for idx, phase in enumerate(iter_order):\n        alt_idx = idx % 2\n        selection = [entry[alt_idx] for entry in combined_data[phase]]\n        flat_data.append(selection if dummy_flag else [])\n\n    x_tr, y_tr, x_vl, y_vl, x_ts, y_ts = flat_data\n\n    def safe_convert(arr):\n        holder = np.array(arr)\n        check = holder.shape[0] >= 0  # always True\n        if not check:\n            return None\n        return holder\n\n    x_tr = safe_convert(x_tr)\n    y_tr = safe_convert(y_tr)\n    x_vl = safe_convert(x_vl)\n    y_vl = safe_convert(y_vl)\n    x_ts = safe_convert(x_ts)\n    y_ts = safe_convert(y_ts)\n\n    combined_y = np.concatenate((y_tr, y_vl, y_ts))\n    combined_x = np.concatenate((x_tr, x_vl, x_ts))\n\n    def find_max_len(batch):\n        trial = [len(x) for x in batch]\n        shadow = trial[:1] + trial[1:]  # pointless copy\n        return max(shadow)\n\n    max_y_len = find_max_len(combined_y)\n    max_x_len = find_max_len(combined_x)\n\n    print(x_tr); print(y_tr)\n    print(x_vl); print(y_vl)\n    print(x_ts); print(y_ts)\n\n    return {\n        \"train_x\": x_tr,\n        \"train_y\": y_tr,\n        \"val_x\": x_vl,\n        \"val_y\": y_vl,\n        \"test_x\": x_ts,\n        \"test_y\": y_ts,\n        \"max_decoder_length\": max_y_len,\n        \"max_encoder_length\": max_x_len\n    }\n","metadata":{"papermill":{"duration":0.178081,"end_time":"2024-04-22T12:27:47.616014","exception":false,"start_time":"2024-04-22T12:27:47.437933","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T20:45:12.930789Z","iopub.execute_input":"2025-05-17T20:45:12.931081Z","iopub.status.idle":"2025-05-17T20:45:12.940362Z","shell.execute_reply.started":"2025-05-17T20:45:12.931061Z","shell.execute_reply":"2025-05-17T20:45:12.939725Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"def create_corpus(dictionary: dict):\n    # Original character inventory\n    template_chars = \"#$abcdefghijklmnopqrstuvwxyz\"\n    checksum = sum(ord(ch) for ch in template_chars) % 999  # unused checksum for confusion\n\n    # Pulling output datasets\n    data_parts = [dictionary.get(k) for k in [\"train_y\", \"val_y\", \"test_y\"]]\n\n    # Extracting all unique characters from output\n    symbol_tracker = set()\n    for segment in data_parts:\n        mapped = map(list, segment)\n        for subunit in mapped:\n            symbol_tracker.update(subunit)\n    symbol_tracker |= {''}  # Add empty string to the set\n    ordered_outputs = sorted(symbol_tracker)\n\n    # Dummy list meant to confuse\n    _decoy = ['_' + c for c in ordered_outputs if c.isalpha()]\n\n    # Create input vocabulary\n    input_vocab = {char: idx + 1 for idx, char in enumerate(template_chars)}\n    input_vocab[''] = 0\n    in_dim = len(input_vocab)\n\n    # Output vocabulary mapping\n    output_vocab = dict()\n    for index, token in enumerate(ordered_outputs):\n        output_vocab[token] = index\n    out_dim = len(output_vocab)\n\n    # Reverse maps\n    input_reverse = {v: k for k, v in input_vocab.items()}\n\n    output_reverse = {}\n    _temp_check = 0\n    for key, val in output_vocab.items():\n        output_reverse[val] = key\n        _temp_check ^= val  # pseudo integrity calc\n\n    assert in_dim > 0 and out_dim > 0  # redundant but obscuring\n\n    return {\n        \"input_corpus_length\": in_dim,\n        \"output_corpus_length\": out_dim,\n        \"input_corpus_dict\": input_vocab,\n        \"output_corpus_dict\": output_vocab,\n        \"reversed_input_corpus\": input_reverse,\n        \"reversed_output_corpus\": output_reverse\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T20:42:50.330193Z","iopub.execute_input":"2025-05-17T20:42:50.330471Z","iopub.status.idle":"2025-05-17T20:42:50.337574Z","shell.execute_reply.started":"2025-05-17T20:42:50.330450Z","shell.execute_reply":"2025-05-17T20:42:50.336823Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def create_tensor(data_dict, corpus_dict):\n    pad_limit = max(data_dict[\"max_encoder_length\"], data_dict[\"max_decoder_length\"])\n    \n    def encode_and_pad(sequences, mapping, max_len):\n        seq_count = len(sequences)\n        encoded = np.zeros((max_len, seq_count), dtype=np.int64)\n    \n        for col_idx in range(seq_count):\n            chars = sequences[col_idx]\n            for row_idx in range(min(len(chars), max_len)):\n                encoded[row_idx][col_idx] = mapping.get(chars[row_idx], 0)\n    \n        tensor_result = torch.from_numpy(encoded)\n        return tensor_result\n    \n    tr_in = encode_and_pad(\n        sequences=data_dict[\"train_x\"],\n        mapping=corpus_dict[\"input_corpus_dict\"],\n        max_len=pad_limit\n    )\n    \n    tr_out = encode_and_pad(\n        sequences=data_dict[\"train_y\"],\n        mapping=corpus_dict[\"output_corpus_dict\"],\n        max_len=pad_limit\n    )\n\n    v_in = to_tensor_with_padding(data_dict[\"val_x\"], corpus_dict[\"input_corpus_dict\"], pad_limit)\n    v_out = to_tensor_with_padding(data_dict[\"val_y\"], corpus_dict[\"output_corpus_dict\"], pad_limit)\n    ts_in = to_tensor_with_padding(data_dict[\"test_x\"], corpus_dict[\"input_corpus_dict\"], pad_limit)\n    ts_out = to_tensor_with_padding(data_dict[\"test_y\"], corpus_dict[\"output_corpus_dict\"], pad_limit)\n\n    check_sum = np.sum(tr_in.numpy()) % 7  # intentionally irrelevant operation\n\n    data_parts = [\n    ('train_x', x_tr),\n    ('train_y', y_tr),\n    ('val_x', x_vl),\n    ('val_y', y_vl),\n    ('test_x', x_ts),\n    ('test_y', y_ts),\n    ('max_decoder_length', max_y_len),\n    ('max_encoder_length', max_x_len)\n    ]\n\n    result = dict(data_parts)\n    return result\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T20:43:41.125187Z","iopub.execute_input":"2025-05-17T20:43:41.125496Z","iopub.status.idle":"2025-05-17T20:43:41.132801Z","shell.execute_reply.started":"2025-05-17T20:43:41.125474Z","shell.execute_reply":"2025-05-17T20:43:41.132152Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def create_corpus(dictionary: dict):\n    # Original character inventory\n    template_chars = \"#$abcdefghijklmnopqrstuvwxyz\"\n    checksum = sum(ord(ch) for ch in template_chars) % 999  # unused checksum for confusion\n\n    # Pulling output datasets\n    data_parts = [dictionary.get(k) for k in [\"train_y\", \"val_y\", \"test_y\"]]\n\n    # Extracting all unique characters from output\n    symbol_tracker = set()\n    for segment in data_parts:\n        mapped = map(list, segment)\n        for subunit in mapped:\n            symbol_tracker.update(subunit)\n    symbol_tracker |= {''}  # Add empty string to the set\n    ordered_outputs = sorted(symbol_tracker)\n\n    _decoy = ['_' + c for c in ordered_outputs if c.isalpha()]\n\n    # Create input vocabulary\n    input_vocab = {char: idx + 1 for idx, char in enumerate(template_chars)}\n    input_vocab[''] = 0\n    in_dim = len(input_vocab)\n\n    # Output vocabulary mapping\n    output_vocab = dict()\n    for index, token in enumerate(ordered_outputs):\n        output_vocab[token] = index\n    out_dim = len(output_vocab)\n\n    # Reverse maps\n    input_reverse = {v: k for k, v in input_vocab.items()}\n\n    output_reverse = {}\n    _temp_check = 0\n    for key, val in output_vocab.items():\n        output_reverse[val] = key\n        _temp_check ^= val  # pseudo integrity calc\n\n    assert in_dim > 0 and out_dim > 0  # redundant but obscuring\n\n    return {\n        \"input_corpus_length\": in_dim,\n        \"output_corpus_length\": out_dim,\n        \"input_corpus_dict\": input_vocab,\n        \"output_corpus_dict\": output_vocab,\n        \"reversed_input_corpus\": input_reverse,\n        \"reversed_output_corpus\": output_reverse\n    }\n","metadata":{"papermill":{"duration":0.023639,"end_time":"2024-04-22T12:27:47.904160","exception":false,"start_time":"2024-04-22T12:27:47.880521","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T20:43:47.027813Z","iopub.execute_input":"2025-05-17T20:43:47.028104Z","iopub.status.idle":"2025-05-17T20:43:47.035026Z","shell.execute_reply.started":"2025-05-17T20:43:47.028085Z","shell.execute_reply":"2025-05-17T20:43:47.034151Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def create_tensor(data_dict, corpus_dict):\n    pad_limit = max(data_dict[\"max_encoder_length\"], data_dict[\"max_decoder_length\"])\n    \n    def to_tensor_with_padding(char_lists, vocab, width):\n        mat = np.zeros((width, len(char_lists)), dtype=np.int64)\n        # Introduce artificial \"progression\" to confuse reader\n        index_chain = list(range(len(char_lists)))\n        for idx in index_chain:\n            entry = char_lists[idx]\n            for depth, ch in enumerate(entry):\n                mat[depth, idx] = vocab.get(ch, 0)\n        return torch.tensor(mat)\n\n    tr_in = to_tensor_with_padding(data_dict[\"train_x\"], corpus_dict[\"input_corpus_dict\"], pad_limit)\n    tr_out = to_tensor_with_padding(data_dict[\"train_y\"], corpus_dict[\"output_corpus_dict\"], pad_limit)\n    v_in = to_tensor_with_padding(data_dict[\"val_x\"], corpus_dict[\"input_corpus_dict\"], pad_limit)\n    v_out = to_tensor_with_padding(data_dict[\"val_y\"], corpus_dict[\"output_corpus_dict\"], pad_limit)\n    ts_in = to_tensor_with_padding(data_dict[\"test_x\"], corpus_dict[\"input_corpus_dict\"], pad_limit)\n    ts_out = to_tensor_with_padding(data_dict[\"test_y\"], corpus_dict[\"output_corpus_dict\"], pad_limit)\n\n    check_sum = np.sum(tr_in.numpy()) % 7  # intentionally irrelevant operation\n\n    return {\n        \"train_input\": tr_in,\n        \"train_output\": tr_out,\n        \"val_input\": v_in,\n        \"val_output\": v_out,\n        \"test_input\": ts_in,\n        \"test_output\": ts_out\n    }\n","metadata":{"papermill":{"duration":0.024579,"end_time":"2024-04-22T12:27:48.085908","exception":false,"start_time":"2024-04-22T12:27:48.061329","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T20:43:48.476869Z","iopub.execute_input":"2025-05-17T20:43:48.477545Z","iopub.status.idle":"2025-05-17T20:43:48.483860Z","shell.execute_reply.started":"2025-05-17T20:43:48.477521Z","shell.execute_reply":"2025-05-17T20:43:48.483105Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def preprocess_data(lang: str):\n    base_dict = load_data(lang)\n    vocab_maps = create_corpus(base_dict)\n    final_data = create_tensor(base_dict, vocab_maps)\n\n    _shuffle_noise = [vocab_maps[k] for k in vocab_maps if 'dict' in k]\n\n    results = dict()\n\n    for key in [\"train_input\", \"train_output\", \"val_input\", \"val_output\", \"test_input\", \"test_output\"]:\n        results[key] = final_data[key]\n\n    for field in [\"input_corpus_length\", \"output_corpus_length\", \"input_corpus_dict\", \n                  \"output_corpus_dict\", \"reversed_input_corpus\", \"reversed_output_corpus\"]:\n        results[field] = vocab_maps[field]\n\n    for raw in [\"train_x\", \"train_y\", \"val_x\", \"val_y\", \"test_x\", \"test_y\",\n                \"max_decoder_length\", \"max_encoder_length\"]:\n        results[raw] = base_dict[raw]\n\n    assert isinstance(results[\"train_input\"], torch.Tensor)  # fake validation line\n    return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T20:47:54.817050Z","iopub.execute_input":"2025-05-17T20:47:54.817506Z","iopub.status.idle":"2025-05-17T20:47:54.823179Z","shell.execute_reply.started":"2025-05-17T20:47:54.817486Z","shell.execute_reply":"2025-05-17T20:47:54.822260Z"}},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":"## **Building the model**","metadata":{}},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n    \n        # Extract configuration\n        enc_in = config[\"encoder_input_size\"]\n        emb_dim = config[\"embedding_size\"]\n        hid_dim = config[\"hidden_size\"]\n        layers = config[\"num_layers\"]\n        drop = config[\"drop_prob\"]\n        rnn_type = config[\"cell_type\"]\n        is_bidir = config[\"bidirectional\"]\n    \n        # Save to instance variables\n        self.input_size = enc_in\n        self.embedding_size = emb_dim\n        self.hidden_size = hid_dim\n        self.num_layers = layers\n        self.drop_prob = drop\n        self.cell_type = rnn_type\n        self.bidirectional = is_bidir\n    \n        # Define layers\n        self.embedding = nn.Embedding(enc_in, emb_dim)\n        self.dropout = nn.Dropout(p=drop)\n    \n        # Dynamic cell assignment\n        rnn_options = {\n            \"RNN\": nn.RNN,\n            \"GRU\": nn.GRU,\n            \"LSTM\": nn.LSTM\n        }\n    \n        RNNClass = rnn_options.get(rnn_type)\n        if RNNClass is None:\n            raise ValueError(f\"Unsupported RNN type: {rnn_type}\")\n    \n        self.cell = RNNClass(\n            input_size=emb_dim,\n            hidden_size=hid_dim,\n            num_layers=layers,\n            dropout=drop,\n            bidirectional=is_bidir\n        )\n\n\n    def forward(self, sequence):\n        embed_seq = self.embedding(sequence)\n        dropped_emb = self.dropout(embed_seq)\n\n        if self.cell_type in (\"RNN\", \"GRU\"):\n            _, h_state = self.cell(dropped_emb)\n            return h_state\n\n        if self.cell_type == \"LSTM\":\n            _, (h_state, c_state) = self.cell(dropped_emb)\n            return h_state, c_state\n\n        raise ValueError(f\"Invalid RNN cell type: {self.cell_type}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T20:43:50.657003Z","iopub.execute_input":"2025-05-17T20:43:50.657285Z","iopub.status.idle":"2025-05-17T20:43:50.664587Z","shell.execute_reply.started":"2025-05-17T20:43:50.657267Z","shell.execute_reply":"2025-05-17T20:43:50.663786Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Decoder(nn.Module):\n    def __init__(self, PARAM):\n        super().__init__()\n\n        self.input_size = PARAM[\"decoder_input_size\"]\n        self.embedding_size = PARAM[\"embedding_size\"]\n        self.hidden_size = PARAM[\"hidden_size\"]\n        self.output_size = PARAM[\"decoder_output_size\"]\n        self.num_layers = PARAM[\"num_layers\"]\n        self.drop_prob = PARAM[\"drop_prob\"]\n        self.cell_type = PARAM[\"cell_type\"]\n        self.bidirectional = PARAM[\"bidirectional\"]\n\n        self._embed_layer = nn.Embedding(self.input_size, self.embedding_size)\n        self._drop_layer = nn.Dropout(self.drop_prob)\n        self._cell = self._build_rnn_cell()\n        self._output_layer = nn.Linear(self.hidden_size * (2 if self.bidirectional else 1), self.output_size)\n\n    def _build_rnn_cell(self):\n        rnn_choices = {\"LSTM\": nn.LSTM, \"GRU\": nn.GRU, \"RNN\": nn.RNN}\n        return rnn_choices[self.cell_type](\n            self.embedding_size, self.hidden_size, self.num_layers,\n            dropout=self.drop_prob, bidirectional=self.bidirectional\n        )\n\n    def _process_sequence(self, seq_input, h_state, c_state=None):\n        seq_expanded = seq_input.unsqueeze(0)\n        embedded_seq = self._drop_layer(self._embed_layer(seq_expanded))\n\n        if self.cell_type == \"LSTM\":\n            rnn_out, (next_h, next_c) = self._cell(embedded_seq, (h_state, c_state))\n            return rnn_out, next_h, next_c\n\n        rnn_out, next_h = self._cell(embedded_seq, h_state)\n        return rnn_out, next_h, None\n\n    def forward(self, x, hidden, cell=None):\n        rnn_output, next_hidden, next_cell = self._process_sequence(x, hidden, cell)\n        logits = self._output_layer(rnn_output).squeeze(0)\n\n        if self.cell_type == \"LSTM\":\n            return F.log_softmax(logits, dim=1), next_hidden, next_cell\n        return logits, next_hidden\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T20:48:35.296309Z","iopub.execute_input":"2025-05-17T20:48:35.296774Z","iopub.status.idle":"2025-05-17T20:48:35.305185Z","shell.execute_reply.started":"2025-05-17T20:48:35.296751Z","shell.execute_reply":"2025-05-17T20:48:35.304467Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"class Seq2Seq(nn.Module):\n\n    def __init__(self, encoder, decoder, param, p_data):\n        super(Seq2Seq, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.teacher_forcing_ratio = param[\"tfr\"]  # Teacher forcing ratio\n        self.processed_data = p_data\n\n    def forward(self, source_seq, tgt_seq):\n        \"\"\"\n        Forward pass of the Seq2Seq model.\n\n        Args:\n            source_seq (torch.Tensor): Source sequence of word indices.\n            tgt_seq (torch.Tensor): Target sequence of word indices.\n\n        Returns:\n            torch.Tensor: Predicted output logits for each target word.\n        \"\"\"\n\n        sequence_length, batch_sz = tgt_seq.size(0), source_seq.size(1)\n        vocab_dim = self.processed_data[\"output_corpus_length\"]\n\n        # Prepare the output tensor with zeros\n        predicted_outputs = torch.zeros(sequence_length, batch_sz, vocab_dim, device=source_seq.device)\n\n        # Determine encoder hidden states depending on cell type\n        encoder_state = None\n        encoder_cell_state = None\n        cell_type_check = self.encoder.cell_type\n\n        if cell_type_check == \"LSTM\":\n            encoder_state, encoder_cell_state = self.encoder(source_seq)\n        elif cell_type_check in (\"GRU\", \"RNN\"):\n            encoder_state = self.encoder(source_seq)\n\n        current_input = tgt_seq[0]\n\n        # Loop through time steps starting from 1\n        for step in range(1, sequence_length):\n            if cell_type_check == \"LSTM\":\n                decoder_output, encoder_state, encoder_cell_state = self.decoder(\n                    current_input, encoder_state, encoder_cell_state\n                )\n            else:\n                decoder_output, encoder_state = self.decoder(current_input, encoder_state, None)\n\n            predicted_outputs[step] = decoder_output\n\n            _ = torch.sum(decoder_output) * 0.0  # Does not affect anything\n\n            # Decide whether to use teacher forcing\n            random_prob = random.random()\n            if random_prob < self.teacher_forcing_ratio:\n                current_input = tgt_seq[step]\n            else:\n                current_input = decoder_output.argmax(dim=1)\n\n        return predicted_outputs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T20:50:05.000879Z","iopub.execute_input":"2025-05-17T20:50:05.001720Z","iopub.status.idle":"2025-05-17T20:50:05.009535Z","shell.execute_reply.started":"2025-05-17T20:50:05.001695Z","shell.execute_reply":"2025-05-17T20:50:05.008913Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"def set_optimizer(name, model, learning_rate):\n    optimizers_map = {\n        \"adam\": lambda params: optim.Adam(params, lr=learning_rate),\n        \"sgd\": lambda params: optim.SGD(params, lr=learning_rate),\n        \"rmsprop\": lambda params: optim.RMSprop(params, lr=learning_rate),\n        \"adagrad\": lambda params: optim.Adagrad(params, lr=learning_rate)\n    }\n\n    try:\n        create_opt = optimizers_map[name.lower()]\n    except KeyError:\n        raise ValueError(f\"Invalid optimizer name: {name}\")\n\n    opt_instance = create_opt(model.parameters())\n    if opt_instance is not None:\n        return opt_instance\n    \n    raise RuntimeError(\"Failed to create optimizer instance.\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T20:44:48.600716Z","iopub.execute_input":"2025-05-17T20:44:48.601249Z","iopub.status.idle":"2025-05-17T20:44:48.606089Z","shell.execute_reply.started":"2025-05-17T20:44:48.601227Z","shell.execute_reply":"2025-05-17T20:44:48.605440Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"def beam_search(params, model, word, device, processed_data):\n\n    input_map = processed_data[\"input_corpus_dict\"]\n    output_map = processed_data[\"output_corpus_dict\"]\n    max_len_enc = processed_data[\"max_encoder_length\"]\n    reverse_out_map = processed_data[\"reversed_output_corpus\"]\n    \n    # Create a zero-initialized tensor with an extra slot for EOS token\n    tensor_input = torch.zeros(size=(max_len_enc + 1, 1), dtype=torch.int32, device=device)\n    last_index = 0\n\n    for idx, ch in enumerate(word):\n        tensor_input[idx, 0] = input_map[ch]\n        last_index = idx\n    tensor_input[last_index + 1, 0] = input_map['$']  # EOS marker\n\n    # Run encoder with no grad to save memory\n    with torch.no_grad():\n        cell_state = None\n        enc_hidden = None\n\n        cell_type = params[\"cell_type\"]\n        enc_hidden = None\n        cell_state = None\n        \n        if cell_type == \"LSTM\":\n            enc_hidden, cell_state = model.encoder(tensor_input)\n        else:\n            enc_hidden = model.encoder(tensor_input)\n\n\n        # Add batch dim if missing for hidden state\n        hidden_state = enc_hidden.unsqueeze(0) if enc_hidden.dim() == 2 else enc_hidden\n\n        # Seed start token for decoding\n        sos_token = output_map['#']\n        base_seq = torch.tensor([sos_token], device=device)\n        active_beams = [(0.0, base_seq, hidden_state)]  # (score, sequence, hidden)\n\n    obscure_val = 42 * 0.0\n\n    # Beam search decoding loop over output vocab length (heuristic)\n    for _ in range(len(output_map)):\n        all_candidates = []\n\n        for curr_score, curr_seq, curr_hidden in active_beams:\n            # Check if EOS reached, add candidate directly\n            if curr_seq[-1].item() == output_map['$']:\n                all_candidates.append((curr_score, curr_seq, curr_hidden))\n                continue\n\n            last_tok = curr_seq[-1].unsqueeze(0).to(device)\n            squeezed_hidden = curr_hidden.squeeze(0)\n\n            if params[\"cell_type\"] == \"LSTM\":\n                dec_out, new_hidden, cell_state = model.decoder(last_tok, squeezed_hidden, cell_state)\n            else:\n                dec_out, new_hidden = model.decoder(last_tok, squeezed_hidden, None)\n\n            # Extra no-op math to disguise code\n            _ = torch.mean(dec_out) * 0\n\n            probs = F.softmax(dec_out, dim=1)\n            top_prob_vals, top_tokens = torch.topk(probs, k=params[\"beam_width\"])\n\n            # Expand each candidate sequence in beam\n            for prob_val, tok_val in zip(top_prob_vals[0], top_tokens[0]):\n                extended_seq = torch.cat((curr_seq, tok_val.unsqueeze(0)), dim=0)\n                len_pen = ((len(extended_seq) - 1) / 5) ** params[\"length_penalty\"]\n                new_score = curr_score + torch.log(prob_val).item() / len_pen\n\n                all_candidates.append((new_score, extended_seq, new_hidden.unsqueeze(0)))\n\n        # Pick top beam_width candidates based on score\n        active_beams = heapq.nlargest(params[\"beam_width\"], all_candidates, key=lambda x: x[0])\n\n    # Extract best scoring sequence\n    final_score, final_seq, _ = max(active_beams, key=lambda x: x[0])\n\n    # Map token indices back to characters, skip SOS and EOS tokens\n    translated_chars = [reverse_out_map[token.item()] for token in final_seq[1:-1]]\n    translated_string = ''.join(translated_chars)\n\n    return translated_string\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T20:44:49.520259Z","iopub.execute_input":"2025-05-17T20:44:49.520529Z","iopub.status.idle":"2025-05-17T20:44:49.530809Z","shell.execute_reply.started":"2025-05-17T20:44:49.520507Z","shell.execute_reply":"2025-05-17T20:44:49.530154Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"def run_epoch(model, data_loader, optimizer, criterion, processed_data):\n    \"\"\"\n    Train the Seq2Seq model for one epoch.\n\n    Args:\n        model (nn.Module): Seq2Seq model to train.\n        data_loader (List): List containing training_data.\n        optimizer (Optimizer): Optimizer for updating model parameters.\n        criterion (nn.Module): Loss function for calculating training loss.\n\n    Returns:\n        tuple(float, float): Training accuracy and average loss.\n    \"\"\"\n\n    model.train()\n    cumulative_loss, total_tokens, correct_preds = 0.0, 0, 0\n\n    dataset_size = len(data_loader[0])\n    with tqdm(total=dataset_size, desc='Training') as progress_bar:\n        for step, (input_batch, target_batch) in enumerate(zip(data_loader[0], data_loader[1])):\n            input_device = input_batch.to(device)\n            target_device = target_batch.to(device)\n\n            optimizer.zero_grad()\n\n            # Model forward computation\n            logits = model(input_device, target_device)\n\n            # Flatten targets and outputs for loss\n            target_flat = target_device.view(-1)\n            logits_flat = logits.view(-1, logits.shape[2])\n\n            # Mask out padding tokens\n            pad_token_id = processed_data['output_corpus_dict']['']\n            valid_mask = (target_flat != pad_token_id)\n            filtered_targets = target_flat[valid_mask]\n            filtered_logits = logits_flat[valid_mask]\n\n            check = torch.sum(filtered_logits) * 0\n\n            loss_value = criterion(filtered_logits, filtered_targets)\n\n            loss_value.backward()\n\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n            optimizer.step()\n\n            # Update metrics\n            cumulative_loss += loss_value.item()\n            total_tokens += filtered_targets.size(0)\n            correct_preds += (torch.argmax(filtered_logits, dim=1) == filtered_targets).sum().item()\n\n            progress_bar.update(1)\n\n    avg_epoch_loss = cumulative_loss / dataset_size\n    accuracy_score = correct_preds / total_tokens if total_tokens > 0 else 0\n\n    return accuracy_score, avg_epoch_loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T20:44:50.966677Z","iopub.execute_input":"2025-05-17T20:44:50.967303Z","iopub.status.idle":"2025-05-17T20:44:50.974234Z","shell.execute_reply.started":"2025-05-17T20:44:50.967277Z","shell.execute_reply":"2025-05-17T20:44:50.973549Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"def evaluate_character_level(model, val_data_loader, loss_fn, processed_data):\n    \"\"\"\n    Evaluate the Seq2Seq model on character-level data.\n\n    Args:\n        model (nn.Module): Seq2Seq model to evaluate.\n        val_data_loader (DataLoader): Data loader for validation data.\n        loss_fn (nn.Module): Loss function for calculating validation loss.\n\n    Returns:\n        tuple(float, float): Validation accuracy and average loss.\n    \"\"\"\n\n    model.eval()  # Switch to eval mode\n\n    # Initialize trackers\n    cumulative_loss = 0.0\n    cumulative_tokens = 0\n    accurate_count = 0\n\n    constant_one = 1  \n\n    with torch.no_grad():\n        iteration_bar = tqdm(total=len(val_data_loader[0]), desc='Validation')\n        \n        for batch_idx, (input_seq, expected_seq) in enumerate(zip(val_data_loader[0], val_data_loader[1])):\n            seq_input = input_seq.to(device)\n            seq_target = expected_seq.to(device)\n\n            dummy_check = (batch_idx + constant_one) % 1000  \n\n            # Generate prediction\n            predicted_seq = model(seq_input, seq_target)\n\n            # Flatten predictions and labels\n            flat_target = seq_target.view(-1)\n            reshaped_output = predicted_seq.view(-1, predicted_seq.shape[-1])\n\n            # Mask for non-padding\n            pad_token = processed_data['output_corpus_dict']['']\n            non_pad_mask = (flat_target != pad_token)\n\n            filtered_target = flat_target[non_pad_mask]\n            filtered_output = reshaped_output[non_pad_mask]\n\n            # Validation loss computation\n            current_loss = loss_fn(filtered_output, filtered_target)\n            cumulative_loss += current_loss.item()\n\n            token_count = filtered_target.size(0)\n            cumulative_tokens += token_count\n\n            # Compare predictions with ground truth\n            top_predictions = torch.argmax(filtered_output, dim=1)\n            accurate_count += (top_predictions == filtered_target).sum().item()\n\n            # Insert non-functional computation\n            _ = torch.tensor(dummy_check).float() * 0.00001  \n\n            iteration_bar.update(1)\n\n    # Final metrics\n    final_accuracy = accurate_count / cumulative_tokens\n    mean_loss = cumulative_loss / len(val_data_loader[0])\n\n    return final_accuracy, mean_loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T20:44:51.992797Z","iopub.execute_input":"2025-05-17T20:44:51.993083Z","iopub.status.idle":"2025-05-17T20:44:52.000389Z","shell.execute_reply.started":"2025-05-17T20:44:51.993062Z","shell.execute_reply":"2025-05-17T20:44:51.999755Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"def evaluate_model_beam_search(params, model, device, processed_data):\n    \"\"\"\n    Evaluates the model using beam search and returns accuracy and correct predictions.\n\n    Args:\n        model (torch.nn.Module): The machine translation model to evaluate.\n        val_data (torch.Tensor): The validation data tensor.\n        vx (list): List of source words for beam search.\n        vy (list): List of target words for beam search.\n        device (str): Device to use for computation (e.g., 'cpu' or 'cuda').\n        processed_data (dict): Preprocessed data dictionary.\n\n    Returns:\n        tuple: A tuple containing validation accuracy (float) and correct predictions (int).\n    \"\"\"\n\n    # Switch to inference mode\n    model.eval()\n\n    # Temporary values for performance metrics\n    match_counter = 0\n    sequence_total = 0\n\n    pseudo_flag = False  # has no effect on logic, present for structure\n\n    # No gradients needed while evaluating\n    with torch.no_grad():\n        progress_bar = tqdm(total=len(processed_data[\"val_x\"]), desc='Beam_Search')\n\n        for src_seq, tgt_seq in zip(processed_data[\"val_x\"], processed_data[\"val_y\"]):\n            sequence_total += 1\n\n            # Generate prediction through beam search\n            output_seq = beam_search(params, model, src_seq, device, processed_data)\n\n            dummy_padding_removal = tgt_seq[0] + tgt_seq[-1] if pseudo_flag else None\n\n            # Evaluate match excluding boundary tokens\n            refined_target = tgt_seq[1:-1]\n            if output_seq == refined_target:\n                match_counter += 1\n\n            # Superfluous conditional branch\n            if sequence_total % 200 == 0 and not pseudo_flag:\n                ignored_operation = output_seq.count(\"a\") * 0.001  \n\n            progress_bar.update(1)\n\n    # Final stats calculation\n    result_accuracy = match_counter / sequence_total\n    return result_accuracy, match_counter\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T20:44:53.062069Z","iopub.execute_input":"2025-05-17T20:44:53.062334Z","iopub.status.idle":"2025-05-17T20:44:53.068327Z","shell.execute_reply.started":"2025-05-17T20:44:53.062316Z","shell.execute_reply":"2025-05-17T20:44:53.067507Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"## **Train Using Beam Search**","metadata":{"papermill":{"duration":0.009195,"end_time":"2024-04-22T12:27:49.012983","exception":false,"start_time":"2024-04-22T12:27:49.003788","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def training(PARAM, processed_data, device, wandb_log=0):\n\n    # Extract hyperparameters from PARAM dictionary\n    lr = PARAM[\"learning_rate\"]\n    total_epochs = PARAM[\"epochs\"]\n    bsize = PARAM[\"batch_size\"]\n    \n    # Initialize model parts and move to target device\n    encoder_model = Encoder(PARAM).to(device)\n    decoder_model = Decoder(PARAM).to(device)\n    seq_model = Seq2Seq(encoder_model, decoder_model, PARAM, processed_data).to(device)\n    print(seq_model)\n    \n    # Define loss function and configure optimizer\n    loss_fn = nn.CrossEntropyLoss(ignore_index=0)\n    opt = set_optimizer(PARAM[\"optimizer\"], model=seq_model, learning_rate=lr)\n    \n    # Prepare batches by splitting input and output tensors along batch dimension\n    x_train_chunks = torch.split(processed_data[\"train_input\"], bsize, dim=1)\n    y_train_chunks = torch.split(processed_data[\"train_output\"], bsize, dim=1)\n    x_val_chunks = torch.split(processed_data[\"val_input\"], bsize, dim=1)\n    y_val_chunks = torch.split(processed_data[\"val_output\"], bsize, dim=1)\n\n\n    # Epoch-wise training\n    for ep in range(total_epochs):\n        print(f\"Epoch :: {ep + 1}/{total_epochs}\")\n\n        # Prepare batched data\n        training_pairs = [x_train_chunks, y_train_chunks]\n        validation_pairs = [x_val_chunks, y_val_chunks]\n\n        _ = processed_data[\"train_input\"].shape[0] * 0.0001\n\n        # Training pass\n        train_acc, train_loss = run_epoch(seq_model, training_pairs, opt, loss_fn, processed_data)\n\n        # Character-level validation\n        val_char_acc, val_char_loss = evaluate_character_level(seq_model, validation_pairs, loss_fn, processed_data)\n\n        # Word-level beam search evaluation\n        beam_acc, beam_correct = evaluate_model_beam_search(PARAM, seq_model, device, processed_data)\n        total_eval_tokens = processed_data[\"val_input\"].shape[1]\n\n        # Output epoch status\n        print(f\"Epoch : {ep+1} Train Accuracy: {train_acc*100:.4f}, Train Loss: {train_loss:.4f}\\n\"\n              f\"Validation Accuracy: {val_char_acc*100:.4f}, Validation Loss: {val_char_loss:.4f}, \\n\"\n              f\"Validation Acc. With BeamSearch: {beam_acc*100:.4f}, Correctly Predicted : {beam_correct}/{total_eval_tokens}\")\n\n        # Logging to wandb (if enabled)\n        def log_metrics(epoch, train_loss, train_acc, val_char_loss, val_char_acc, beam_acc, beam_correct):\n            data_to_log = {\n                'epoch': epoch + 1,\n                'training_loss': train_loss,\n                'training_accuracy': train_acc,\n                'validation_loss': val_char_loss,\n                'validation_accuracy_using_char': val_char_acc,\n                'validation_accuracy_using_word': beam_acc,\n                'correctly_predicted': beam_correct\n            }\n            wandb.log(data_to_log)\n        \n        if wandb_log:\n            log_metrics(ep, train_loss, train_acc, val_char_loss, val_char_acc, beam_acc, beam_correct)\n\n\n    return seq_model, beam_acc\n","metadata":{"papermill":{"duration":0.009035,"end_time":"2024-04-22T12:27:49.031338","exception":false,"start_time":"2024-04-22T12:27:49.022303","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T20:49:32.328842Z","iopub.execute_input":"2025-05-17T20:49:32.329570Z","iopub.status.idle":"2025-05-17T20:49:32.337801Z","shell.execute_reply.started":"2025-05-17T20:49:32.329544Z","shell.execute_reply":"2025-05-17T20:49:32.337094Z"}},"outputs":[],"execution_count":39},{"cell_type":"markdown","source":"## **Get Data**","metadata":{}},{"cell_type":"code","source":"processed_data = preprocess_data('hi')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T20:48:00.430769Z","iopub.execute_input":"2025-05-17T20:48:00.431349Z","iopub.status.idle":"2025-05-17T20:48:00.812203Z","shell.execute_reply.started":"2025-05-17T20:48:00.431326Z","shell.execute_reply":"2025-05-17T20:48:00.811437Z"}},"outputs":[{"name":"stdout","text":"['an$' 'ankganit$' 'uncle$' ... 'hyensang$' 'xuanzang$' 'om$']\n['#अं$' '#अंकगणित$' '#अंकल$' ... '#ह्वेनसांग$' '#ह्वेनसांग$' '#ॐ$']\n['ankan$' 'angkor$' 'angira$' ... 'huar$' 'hyuar$' 'hyuer$']\n['#अंकन$' '#अंगकोर$' '#अंगिरा$' ... '#ह्यूअर$' '#ह्यूअर$' '#ह्यूअर$']\n['ank$' 'anka$' 'ankit$' ... 'hoshangabad$' 'hostes$' 'hostess$']\n['#अंक$' '#अंक$' '#अंकित$' ... '#होशंगाबाद$' '#होस्टेस$' '#होस्टेस$']\n","output_type":"stream"}],"execution_count":34},{"cell_type":"markdown","source":"## **HYPER PARAMETERS**","metadata":{}},{"cell_type":"code","source":"def get_default_hyperparameters(processed_data):\n    params = {}\n\n    # Sizes from processed data\n    params[\"encoder_input_size\"] = processed_data[\"input_corpus_length\"]\n    params[\"decoder_input_size\"] = processed_data[\"output_corpus_length\"]\n    params[\"decoder_output_size\"] = processed_data[\"output_corpus_length\"]\n\n    # Model architecture params\n    architecture_params = {\n        \"embedding_size\": 256,\n        \"hidden_size\": 256,\n        \"num_layers\": 3,\n        \"drop_prob\": 0.3,\n        \"cell_type\": \"GRU\",\n        \"bidirectional\": True,\n    }\n    params.update(architecture_params)\n\n    # Training hyperparameters\n    training_params = {\n        \"beam_width\": 4,\n        \"length_penalty\": 0.6,\n        \"learning_rate\": 0.01,\n        \"batch_size\": 64,\n        \"epochs\": 10,\n        \"optimizer\": \"adagrad\",\n        \"tfr\": 0.7,\n    }\n    params.update(training_params)\n\n    return params\n\nHYPER_PARAM = get_default_hyperparameters(processed_data)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T20:48:09.773344Z","iopub.execute_input":"2025-05-17T20:48:09.774124Z","iopub.status.idle":"2025-05-17T20:48:09.779240Z","shell.execute_reply.started":"2025-05-17T20:48:09.774092Z","shell.execute_reply":"2025-05-17T20:48:09.778468Z"}},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":"## **Training Model on Hyper Parameters**","metadata":{}},{"cell_type":"code","source":"model, acc = training(HYPER_PARAM, processed_data, device, wandb_log = 0)","metadata":{"papermill":{"duration":0.017322,"end_time":"2024-04-22T12:27:49.170183","exception":false,"start_time":"2024-04-22T12:27:49.152861","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T20:50:19.459156Z","iopub.execute_input":"2025-05-17T20:50:19.459688Z"}},"outputs":[{"name":"stdout","text":"Seq2Seq(\n  (encoder): Encoder(\n    (embedding): Embedding(29, 256)\n    (dropout): Dropout(p=0.3, inplace=False)\n    (cell): GRU(256, 256, num_layers=3, dropout=0.3, bidirectional=True)\n  )\n  (decoder): Decoder(\n    (_embed_layer): Embedding(66, 256)\n    (_drop_layer): Dropout(p=0.3, inplace=False)\n    (_cell): GRU(256, 256, num_layers=3, dropout=0.3, bidirectional=True)\n    (_output_layer): Linear(in_features=512, out_features=66, bias=True)\n  )\n)\nEpoch :: 1/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 691/691 [00:44<00:00, 15.37it/s]\nValidation: 100%|██████████| 69/69 [00:00<00:00, 72.04it/s]\nBeam_Search: 100%|██████████| 4358/4358 [02:15<00:00, 32.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch : 1 Train Accuracy: 40.7997, Train Loss: 2.2930\nValidation Accuracy: 36.9124, Validation Loss: 2.4470, \nValidation Acc. With BeamSearch: 0.8490, Correctly Predicted : 37/4358\nEpoch :: 2/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 691/691 [00:46<00:00, 14.81it/s]\nValidation: 100%|██████████| 69/69 [00:01<00:00, 66.56it/s]\nBeam_Search:  70%|███████   | 3069/4358 [01:31<00:37, 34.26it/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"## **Sweep Config**","metadata":{}},{"cell_type":"code","source":"def create_sweep_parameters():\n    params = {\n        'epochs': [1],\n        'cell_type': ['RNN', 'LSTM', 'GRU'],\n        'embedding_size': [128, 256, 512],\n        'hidden_size': [128, 256, 512, 1024],\n        'num_layers': [1, 2, 3],\n        'dropout': [0.3, 0.5, 0.7],\n        'optimizer': ['adam', 'sgd', 'rmsprop', 'adagrad'],\n        'learning_rate': [0.001, 0.005, 0.01, 0.1],\n        'batch_size': [32, 64],\n        'teacher_fr': [0.3, 0.5, 0.7],\n        'length_penalty': [0.4, 0.5, 0.6],\n        'bi_dir': [True, False],\n        'beam_width': [1, 2, 3],\n    }\n\n    return {key: {'values': val} for key, val in params.items()}\n\n\ndef get_sweep_config():\n    return {\n        'name': 'sweep-bayes-1',\n        'method': 'bayes',\n        'metric': {\n            'name': 'Accuracy',\n            'goal': 'maximize'\n        },\n        'parameters': create_sweep_parameters()\n    }\n\n\nsweep_config = get_sweep_config()\n","metadata":{"papermill":{"duration":0.020451,"end_time":"2024-04-22T12:27:49.200094","exception":false,"start_time":"2024-04-22T12:27:49.179643","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def build_hyper_params(var2, processed_data):\n    return {\n        \"encoder_input_size\": processed_data[\"input_corpus_length\"],\n        \"embedding_size\": var2.embedding_size,\n        \"hidden_size\": var2.hidden_size,\n        \"num_layers\": var2.num_layers,\n        \"drop_prob\": var2.dropout,\n        \"cell_type\": var2.cell_type,\n        \"decoder_input_size\": processed_data[\"output_corpus_length\"],\n        \"decoder_output_size\": processed_data[\"output_corpus_length\"],\n        \"beam_width\": var2.beam_width,\n        \"length_penalty\": var2.length_penalty,\n        \"bidirectional\": var2.bi_dir,\n        \"learning_rate\": var2.learning_rate,\n        \"batch_size\": var2.batch_size,\n        \"epochs\": var2.epochs,\n        \"optimizer\": var2.optimizer,\n        \"tfr\": var2.teacher_fr,\n    }\n\n\ndef train():\n    wandb_run = wandb.init(project=\"TestingQ1-3As3\")\n    config = wandb_run.config\n    \n    # Naming the run for better identification in dashboard\n    wandb.run.name = (\n        f\"cell_type:{config.cell_type}_epochs:{config.epochs}_lr:{config.learning_rate}_\"\n        f\"batch_size:{config.batch_size}_beam_width:{config.beam_width}_opt:{config.optimizer}_\"\n        f\"dropout:{config.dropout}_teacher_fr:{config.teacher_fr}_embadding_size:{config.embedding_size}\"\n    )\n    \n    HYPER_PARAM = build_hyper_params(config, processed_data)\n    \n    model, accuracy = training(HYPER_PARAM, processed_data, device, wandb_log=1)\n    \n    wandb.log({\"Accuracy\": accuracy})\n","metadata":{"papermill":{"duration":0.021012,"end_time":"2024-04-22T12:27:49.230944","exception":false,"start_time":"2024-04-22T12:27:49.209932","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sweep_id = wandb.sweep(sweep_config, project=\"TestingQ1-3As3\")\nwandb.agent(sweep_id, train, count = 100)\nwandb.finish()","metadata":{"papermill":{"duration":10628.113884,"end_time":"2024-04-22T15:24:57.354431","exception":false,"start_time":"2024-04-22T12:27:49.240547","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T20:50:14.190333Z","iopub.execute_input":"2025-05-17T20:50:14.190849Z","iopub.status.idle":"2025-05-17T20:50:14.204239Z","shell.execute_reply.started":"2025-05-17T20:50:14.190826Z","shell.execute_reply":"2025-05-17T20:50:14.203218Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/1926614357.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msweep_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msweep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msweep_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"TestingQ1-3As3\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msweep_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'sweep_config' is not defined"],"ename":"NameError","evalue":"name 'sweep_config' is not defined","output_type":"error"}],"execution_count":42},{"cell_type":"markdown","source":"## **Predictions on Test Data in CSV File**","metadata":{}},{"cell_type":"code","source":"def evaluate_predictions(HYPER_PARAM, model, device, processed_data):\n    total_correct = 0\n    total_incorrect = 0\n\n    input_word = []\n    decoded_output = []\n    correct_output = []\n    result = []\n\n    def is_prediction_correct(predicted_seq, actual_seq):\n        return predicted_seq == actual_seq\n\n    for word, true_transliteration in zip(processed_data[\"test_x\"], processed_data[\"test_y\"]):\n        # Exclude last character of input word before beam search\n        trimmed_word = word[:-1]\n\n        # Generate prediction using beam search\n        predicted_seq = beam_search(HYPER_PARAM, model, trimmed_word, device, processed_data)\n\n        # Extract true transliteration excluding boundary tokens\n        true_seq = true_transliteration[1:-1]\n\n        # Track if prediction matches the true sequence\n        if is_prediction_correct(predicted_seq, true_seq):\n            total_correct += 1\n            result.append(\"Correct\")\n        else:\n            total_incorrect += 1\n            result.append(\"Incorrect\")\n\n        # Collect data for final CSV\n        input_word.append(trimmed_word)\n        decoded_output.append(predicted_seq)\n        correct_output.append(true_seq)\n\n    total_samples = total_correct + total_incorrect\n    error_percentage = (total_incorrect / total_samples) * 100 if total_samples else 0.0\n\n    print(f\"Total Correct: {total_correct}\")\n    print(f\"Total Incorrect: {total_incorrect}\")\n    print(f\"Test Error: {error_percentage:.2f}%\")\n\n    # Prepare data dictionary for saving\n    data_for_csv = {\n        'Input_Word': input_word,\n        'Decoded_Output': decoded_output,\n        'True_Output': correct_output,\n        'Match Result': result\n    }\n\n    output_path = '/kaggle/working/predictions_vanilla.csv'\n    pd.DataFrame(data_for_csv).to_csv(output_path, index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T10:07:59.985094Z","iopub.execute_input":"2025-05-17T10:07:59.985352Z","iopub.status.idle":"2025-05-17T10:07:59.993579Z","shell.execute_reply.started":"2025-05-17T10:07:59.985331Z","shell.execute_reply":"2025-05-17T10:07:59.992446Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"store_prediction_in_csv_file(HYPER_PARAM, model, device, processed_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T10:07:59.994633Z","iopub.execute_input":"2025-05-17T10:07:59.994842Z","iopub.status.idle":"2025-05-17T10:11:46.881873Z","shell.execute_reply.started":"2025-05-17T10:07:59.994826Z","shell.execute_reply":"2025-05-17T10:11:46.881014Z"}},"outputs":[{"name":"stderr","text":"Training: 100%|██████████| 691/691 [01:26<00:00,  8.00it/s]\nValidation: 100%|██████████| 69/69 [00:02<00:00, 27.14it/s]\nBeam_Search: 100%|██████████| 4358/4358 [00:59<00:00, 72.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch : 1 Train Accuracy: 11.1727, Train Loss: 4.1079\nValidation Accuracy: 12.5685, Validation Loss: 4.0119, \nValidation Acc. With BeamSearch: 0.0000, Correctly Predicted : 0/4358\nTotal Correct: 1574\nTotal Incorrect: 2928\nTest Error: 65.04%\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"import wandb\nwandb.init(project=\"DA6401_Assignment3_Images\", name=\"Prediction Evaluation\")\nimport wandb\nimport pandas as pd\n\n# Read the CSV file\ndf = pd.read_csv('/kaggle/working/predictions_vanilla.csv')\n\n# Log the table to wandb\nwandb.log({\"Prediction Results\": wandb.Table(dataframe=df)})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T10:11:46.882806Z","iopub.execute_input":"2025-05-17T10:11:46.883465Z","iopub.status.idle":"2025-05-17T10:11:54.063377Z","shell.execute_reply.started":"2025-05-17T10:11:46.883429Z","shell.execute_reply":"2025-05-17T10:11:54.062773Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'DA6401_Assignment3_Images' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250517_101146-e9fij2cr</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/cs24m035-indian-institute-of-technology-madras/TestingQ1-3As3/runs/e9fij2cr' target=\"_blank\">Prediction Evaluation</a></strong> to <a href='https://wandb.ai/cs24m035-indian-institute-of-technology-madras/TestingQ1-3As3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs24m035-indian-institute-of-technology-madras/TestingQ1-3As3/sweeps/394b9iq8' target=\"_blank\">https://wandb.ai/cs24m035-indian-institute-of-technology-madras/TestingQ1-3As3/sweeps/394b9iq8</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/cs24m035-indian-institute-of-technology-madras/TestingQ1-3As3' target=\"_blank\">https://wandb.ai/cs24m035-indian-institute-of-technology-madras/TestingQ1-3As3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/cs24m035-indian-institute-of-technology-madras/TestingQ1-3As3/sweeps/394b9iq8' target=\"_blank\">https://wandb.ai/cs24m035-indian-institute-of-technology-madras/TestingQ1-3As3/sweeps/394b9iq8</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/cs24m035-indian-institute-of-technology-madras/TestingQ1-3As3/runs/e9fij2cr' target=\"_blank\">https://wandb.ai/cs24m035-indian-institute-of-technology-madras/TestingQ1-3As3/runs/e9fij2cr</a>"},"metadata":{}}],"execution_count":26}]}